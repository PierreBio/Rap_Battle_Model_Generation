{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Mounting Google Drive - to run when start Google Colab"
      ],
      "metadata": {
        "id": "HDKvPfG20oxB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AE5t-iu1ziQT",
        "outputId": "3b1b5ac8-eb91-4b98-a464-4f7050938511"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "except:\n",
        "    IN_COLAB = False\n",
        "\n",
        "if IN_COLAB:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "    # Import the required libraries\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import time\n",
        "import math\n",
        "\n",
        "import pprint  # for pretty printing\n",
        "pp = pprint.PrettyPrinter()\n",
        "\n",
        "import os\n",
        "\n",
        "# Paths setup\n",
        "if \"IN_COLAB\" in os.environ:\n",
        "    ROOT_PATH = '/content/drive/MyDrive/Colab Notebooks/'\n",
        "else:\n",
        "    ROOT_PATH = './'\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Concatenate batch corpus - no need relunch it unless new pre-processed corpus"
      ],
      "metadata": {
        "id": "PZXHZ4mw0VFa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ArDj4zZsy-Mw",
        "outputId": "f25c443f-ac93-4145-ce02-f1f919c0b532"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Concatenated files to /content/drive/MyDrive/Colab Notebooks/concatenated_preprocessed_corpus.txt\n"
          ]
        }
      ],
      "source": [
        "# List of file paths to concatenate\n",
        "file_paths = [\n",
        "    '/content/drive/MyDrive/Colab Notebooks/corpus_cleaned_lyrics_batch_1.txt',\n",
        "    '/content/drive/MyDrive/Colab Notebooks/corpus_cleaned_lyrics_batch_2.txt',\n",
        "    '/content/drive/MyDrive/Colab Notebooks/corpus_cleaned_lyrics_batch_3.txt',\n",
        "    '/content/drive/MyDrive/Colab Notebooks/corpus_cleaned_lyrics_batch_4.txt',\n",
        "    '/content/drive/MyDrive/Colab Notebooks/corpus_cleaned_lyrics_batch_5.txt',\n",
        "    '/content/drive/MyDrive/Colab Notebooks/corpus_cleaned_lyrics_batch_6.txt',\n",
        "    '/content/drive/MyDrive/Colab Notebooks/corpus_cleaned_lyrics_batch_7.txt',\n",
        "    '/content/drive/MyDrive/Colab Notebooks/corpus_cleaned_lyrics_batch_8.txt',\n",
        "    '/content/drive/MyDrive/Colab Notebooks/corpus_cleaned_lyrics_batch_9.txt'\n",
        "]\n",
        "# Output file path\n",
        "CORPUS_UNCLEANED_FILE_PATH = '/content/drive/MyDrive/Colab Notebooks/concatenated_preprocessed_corpus.txt'\n",
        "\n",
        "# Function to concatenate files\n",
        "def concatenate_files(file_paths, output_file_path):\n",
        "    with open(output_file_path, 'w') as output_file:\n",
        "        for file_path in file_paths:\n",
        "            with open(file_path, 'r') as input_file:\n",
        "                content = input_file.read()\n",
        "                output_file.write(content)\n",
        "\n",
        "# Call the function to concatenate files\n",
        "concatenate_files(file_paths, CORPUS_UNCLEANED_FILE_PATH)\n",
        "\n",
        "print(f'Concatenated files to {CORPUS_UNCLEANED_FILE_PATH}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define files path - run each time"
      ],
      "metadata": {
        "id": "pGzrKXNQ0yQn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "iZHxD-GzaLBW"
      },
      "outputs": [],
      "source": [
        "# Path to the initial preprocessed corpus file. This file contains raw data that has undergone\n",
        "# some basic cleaning but may require further processing to be ready for model training.\n",
        "CORPUS_UNCLEANED_FILE_PATH = '/content/drive/MyDrive/Colab Notebooks/concatenated_preprocessed_corpus.txt'\n",
        "\n",
        "# Path to the fully cleaned corpus file. This file is the result of applying an additional layer\n",
        "# of cleaning to the initial preprocessed corpus, making it suitable for more advanced data analysis\n",
        "# and model training.\n",
        "CORPUS_FILE_PATH = '/content/drive/MyDrive/Colab Notebooks/cleaned_corpus.txt'\n",
        "\n",
        "# The directory where various output files like models and processed data will be saved.\n",
        "SAVE_PATH = '/content/drive/MyDrive/Colab Notebooks/save_files/'\n",
        "\n",
        "# Path to the pre-trained Word2Vec model. This model provides embeddings for the words\n",
        "# in the corpus and is used to convert words to vectors for machine learning tasks.\n",
        "WORD2VEC_PATH = '/content/drive/MyDrive/Colab Notebooks/GoogleNews-vectors-negative300.bin'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Extra Corpus cleaning: to run only when extra cleaning is needed\n",
        "\n",
        "additional cleaning steps on a pre-processed corpus of text data:"
      ],
      "metadata": {
        "id": "Q3BpdAgY2nVD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QZzOi30dgtoH"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "This code adds an advanced layer of cleaning to a pre-processed text corpus, specifically tailored for natural language processing applications.\n",
        "The additional cleaning steps include:\n",
        "\n",
        "1. **Further Verse Preprocessing**: It refines each verse in the corpus by removing specific markers (like `<winner>`) and handling special cases\n",
        "such as `<battle_start>`. Unnecessary spaces are also condensed.\n",
        "\n",
        "2. **Word Count Filtering**: Verses are assessed for their word count, and those with a word count below a set threshold (default 10) are excluded.\n",
        "This helps in maintaining a quality standard in the dataset.\n",
        "\n",
        "3. **Corpus Consolidation**: The entire corpus is processed verse by verse, ensuring only those battles (group of verses) that meet the word count\n",
        "criteria are retained. This results in a more compact and relevant dataset.\n",
        "\n",
        "4. **Missing Word Identification**: The code identifies words in the corpus that are not present in a pre-trained Word2Vec model.\n",
        "This is crucial for aligning the corpus vocabulary with the model's vocabulary.\n",
        "\n",
        "5. **Removal of Low-Frequency Missing Words**: Words that are both missing from the Word2Vec model and have low occurrence in the corpus are removed.\n",
        "This step refines the corpus further, eliminating rare or potentially irrelevant words.\n",
        "\n",
        "Overall, these steps significantly enhance the corpusâ€™s quality by ensuring consistency, relevance, and alignment with the Word2Vec model,\n",
        "thereby making it more suitable for training robust NLP models.\n",
        "\"\"\"\n",
        "import re\n",
        "from gensim.models import KeyedVectors\n",
        "\n",
        "# This function preprocesses individual verses in the corpus. It performs operations such as\n",
        "# removing specific markers, consolidating battle start markers, and removing excessive spaces.\n",
        "def preprocess_verse(verse):\n",
        "    verse = verse.replace(\"<eos> <sos>\", \" \")\n",
        "    verse = re.sub(r\"<winner> (challenger|defender|unknown)\", \"\", verse)\n",
        "    verse = verse.replace(\"<battle_start> \", \"<battle_start>\")\n",
        "    verse = re.sub(' +', ' ', verse)\n",
        "    return verse.strip()\n",
        "\n",
        "# Counts the number of words in a verse, used to determine if a battle verse should be kept.\n",
        "def count_words_in_verse(verse):\n",
        "    return len(verse.split())\n",
        "\n",
        "# Determines if a battle should be kept based on the minimum word count criteria.\n",
        "def should_keep_battle(battle, min_words=10):\n",
        "    for verse in battle:\n",
        "        if verse.startswith(\"<sos>\") and count_words_in_verse(verse) < min_words:\n",
        "            return False\n",
        "    return True\n",
        "\n",
        "# Processes the entire corpus, applying the preprocessing rules and filtering out battles\n",
        "# that don't meet the word count criteria.\n",
        "def process_corpus(input_file_path, output_file_path, min_words=10):\n",
        "    with open(input_file_path, 'r', encoding='utf-8') as infile, open(output_file_path, 'w', encoding='utf-8') as outfile:\n",
        "        current_battle = []\n",
        "        current_verse = \"\"\n",
        "\n",
        "        for line in infile:\n",
        "            line = line.strip()\n",
        "\n",
        "            if line == \"<battle_start>\":\n",
        "                if current_battle and should_keep_battle(current_battle, min_words):\n",
        "                    outfile.write('\\n'.join(current_battle) + '\\n')\n",
        "                current_battle = [line]\n",
        "                current_verse = \"\"\n",
        "            elif line in [\"<challenger>\", \"<defender>\", \"<battle_end>\"]:\n",
        "                if current_verse:\n",
        "                    current_battle.append(preprocess_verse(current_verse))\n",
        "                    current_verse = \"\"\n",
        "                current_battle.append(line)\n",
        "            else:\n",
        "                current_verse += line + \" \"\n",
        "\n",
        "        if current_battle and should_keep_battle(current_battle, min_words):\n",
        "            outfile.write('\\n'.join(current_battle) + '\\n')\n",
        "\n",
        "# Paths to the input and output files for processing the corpus.\n",
        "output_file_path = '/content/drive/MyDrive/Colab Notebooks/final_preprocessed_corpus.txt'\n",
        "process_corpus(CORPUS_UNCLEANED_FILE_PATH, output_file_path)\n",
        "\n",
        "# Preprocesses text by converting to lowercase, removing punctuation and special characters,\n",
        "# and splitting into words.\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'\\W+', ' ', text)\n",
        "    return text.strip().split()\n",
        "\n",
        "# Finds words in the corpus that are missing from the Word2Vec model, with their occurrence counts.\n",
        "def find_missing_words_with_counts(corpus_path, word2vec_model):\n",
        "    missing_words_counts = {}\n",
        "\n",
        "    with open(corpus_path, 'r', encoding='utf-8') as file:\n",
        "        for line in file:\n",
        "            words = preprocess_text(line)\n",
        "            for word in words:\n",
        "                if word not in word2vec_model:\n",
        "                    missing_words_counts[word] = missing_words_counts.get(word, 0) + 1\n",
        "\n",
        "    return missing_words_counts\n",
        "\n",
        "# Removes words from the corpus that are missing in the Word2Vec model and occur less frequently\n",
        "# than a specified threshold.\n",
        "def remove_low_frequency_missing_words(corpus_path, output_path, missing_words, min_occurrence=10):\n",
        "    with open(corpus_path, 'r', encoding='utf-8') as infile, open(output_path, 'w', encoding='utf-8') as outfile:\n",
        "        for line in infile:\n",
        "            words = preprocess_text(line)\n",
        "            cleaned_line = ' '.join([word for word in words if missing_words.get(word, 0) >= min_occurrence])\n",
        "            if cleaned_line:\n",
        "                outfile.write(cleaned_line + '\\n')\n",
        "\n",
        "# Path to the word2vec model.\n",
        "word2vec = KeyedVectors.load_word2vec_format(WORD2VEC_PATH, binary=True)\n",
        "\n",
        "corpus_path = '/content/drive/MyDrive/Colab Notebooks/final_preprocessed_corpus.txt'\n",
        "missing_words_counts = find_missing_words_with_counts(corpus_path, word2vec)\n",
        "low_freq_missing_words = {word: count for word, count in missing_words_counts.items() if count < 10}\n",
        "\n",
        "# Path to the new, cleaned corpus file.\n",
        "remove_low_frequency_missing_words(corpus_path, CORPUS_FILE_PATH, low_freq_missing_words)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Word Tokenizer - to run each time"
      ],
      "metadata": {
        "id": "RO_7ClPH5U8h"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d6T8nczaWskz"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "This code defines an `EnhancedWordLevelTokenizer` class for creating a custom word-level tokenizer. Its primary functions include:\n",
        "\n",
        "1. **Training on a corpus:** It learns a mapping from words to indices based on the frequency of words in a given text corpus.\n",
        "2. **Handling special tokens:** It can recognize and prioritize special tokens like `<sos>`, `<eos>`, etc.\n",
        "3. **Encoding and decoding:** Converts texts into sequences of indices and vice versa, facilitating the processing of text data for machine learning models.\n",
        "4. **State preservation:** The tokenizer's configuration can be saved to and loaded from a file, ensuring consistency across different sessions or applications.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "from collections import Counter\n",
        "\n",
        "# Class Definition: EnhancedWordLevelTokenizer\n",
        "# This class is designed for creating a word-level tokenizer that can handle both standard words and special tokens.\n",
        "# It can train on a corpus, encode and decode texts, and save/load its state for future use.\n",
        "class EnhancedWordLevelTokenizer:\n",
        "    # Initialization Method\n",
        "    # Initializes the tokenizer with word-index and index-word mappings, and accepts special tokens.\n",
        "    def __init__(self, special_tokens=None):\n",
        "        self.word_index = {}  # Dictionary mapping words to indices\n",
        "        self.index_word = {}  # Dictionary mapping indices to words\n",
        "        self.special_tokens = special_tokens or []  # List of special tokens\n",
        "\n",
        "    # Training Method\n",
        "    # Trains the tokenizer on a list of texts. It processes the texts to create a word frequency map.\n",
        "    # Special tokens are added first to ensure they have priority.\n",
        "    def train(self, texts):\n",
        "        word_counts = Counter()\n",
        "        for text in texts:\n",
        "            # Split the text into words and special tokens\n",
        "            words = text.replace('>', '> ').replace('<', ' <').split()\n",
        "            word_counts.update(words)\n",
        "\n",
        "        # Add special tokens to the tokenizer\n",
        "        for token in self.special_tokens:\n",
        "            self.add_word(token)\n",
        "\n",
        "        # Add words to the tokenizer based on their frequency\n",
        "        for word, _ in word_counts.most_common():\n",
        "            self.add_word(word)\n",
        "\n",
        "    # Add Word Method\n",
        "    # Adds a word to the tokenizer's dictionaries if it's not already present.\n",
        "    def add_word(self, word):\n",
        "        if word not in self.word_index:\n",
        "            index = len(self.word_index)\n",
        "            self.word_index[word] = index\n",
        "            self.index_word[index] = word\n",
        "\n",
        "    # Encoding Method\n",
        "    # Converts a text into a sequence of indices representing the words.\n",
        "    def encode(self, text):\n",
        "        # Split the text into words and special tokens for encoding\n",
        "        words = text.replace('>', '> ').replace('<', ' <').split()\n",
        "        return [self.word_index.get(word, self.word_index[\"<unk>\"]) for word in words]\n",
        "\n",
        "    # Decoding Method\n",
        "    # Converts a sequence of indices back into a text string.\n",
        "    def decode(self, indices):\n",
        "        decoded_words = []\n",
        "        for index in indices:\n",
        "            word = self.index_word.get(index, \"<unk>\")\n",
        "            # Add the word or special token to the decoded list\n",
        "            decoded_words.append(word)\n",
        "        return ' '.join(decoded_words).strip()\n",
        "\n",
        "    # Save Method\n",
        "    # Saves the tokenizer's state (word-index mapping) to a file for later use.\n",
        "    def save(self, path):\n",
        "        with open(path, 'w') as file:\n",
        "            for word, index in self.word_index.items():\n",
        "                file.write(f'{word}\\t{index}\\n')\n",
        "\n",
        "    # Load Method\n",
        "    # Loads the tokenizer's state from a file, reconstructing the word-index and index-word mappings.\n",
        "    def load(self, path):\n",
        "        with open(path, 'r') as file:\n",
        "            for line in file:\n",
        "                word, index = line.strip().split('\\t')\n",
        "                self.word_index[word] = int(index)\n",
        "                self.index_word[int(index)] = word\n",
        "\n",
        "# Usage Example\n",
        "# Define special tokens that will be treated uniquely by the tokenizer\n",
        "special_tokens = [\"<sos>\", \"<eos>\", \"<pad>\", \"<unk>\", \"<battle_start>\", \"<challenger>\", \"<defender>\", \"<winner>\", \"<battle_end>\"]\n",
        "tokenizer = EnhancedWordLevelTokenizer(special_tokens=special_tokens)\n",
        "\n",
        "# Read and process the corpus file to prepare for tokenizer training\n",
        "with open(CORPUS_FILE_PATH, 'r', encoding='utf-8') as file:\n",
        "    lines = [line.strip() for line in file if line.strip()]\n",
        "\n",
        "# Train the tokenizer with the processed lines and save its state\n",
        "tokenizer.train(lines)\n",
        "tokenizer.save(os.path.join(SAVE_PATH, 'word_level_tokenizer_rev1.txt'))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Split Corpus train-validation-test"
      ],
      "metadata": {
        "id": "F6vDrlmSXAoi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i_LvzZ18_B3o"
      },
      "outputs": [],
      "source": [
        "# This script is designed to split a text corpus into training, validation, and testing datasets.\n",
        "# It is especially useful for preparing data for machine learning models in natural language processing.\n",
        "\n",
        "def split_corpus(corpus_file, save_path, test_size, valid_size):\n",
        "    \"\"\"\n",
        "    Splits the corpus into training, validation, and testing datasets.\n",
        "\n",
        "    Args:\n",
        "    corpus_file (str): Path to the corpus file.\n",
        "    save_path (str): Directory where the split files will be saved.\n",
        "    test_size (float): Proportion of the dataset to include in the test split.\n",
        "    valid_size (float): Proportion of the training dataset to include in the validation split.\n",
        "\n",
        "    The function reads the entire corpus, splits it into individual battles (assuming each battle ends with '<battle_end>\\n'),\n",
        "    and then divides these battles into training, validation, and test sets based on the specified proportions.\n",
        "    \"\"\"\n",
        "\n",
        "    with open(corpus_file, 'r', encoding='utf-8') as f:\n",
        "        # Read and split the corpus into separate battles\n",
        "        data_cleaned = f.read().strip().split('<battle_end>\\n')\n",
        "\n",
        "    # Calculate the indices for splitting the data\n",
        "    num_battles = len(data_cleaned)\n",
        "    test_idx = int(num_battles * (1 - test_size))\n",
        "    valid_idx = int(test_idx * (1 - valid_size))\n",
        "\n",
        "    # Split the data into training, validation, and testing sets\n",
        "    train_battles = data_cleaned[:valid_idx]\n",
        "    valid_battles = data_cleaned[valid_idx:test_idx]\n",
        "    test_battles = data_cleaned[test_idx:]\n",
        "\n",
        "    # Write each dataset to a separate file\n",
        "    for battles, name in [(train_battles, 'train'), (valid_battles, 'valid'), (test_battles, 'test')]:\n",
        "        with open(os.path.join(save_path, f'{name}.txt'), 'w') as file:\n",
        "            for battle in battles:\n",
        "                file.write(battle + '<battle_end>\\n')  # Append '<battle_end>\\n' to maintain the original format\n",
        "\n",
        "# Example usage of the function\n",
        "split_corpus(CORPUS_FILE_PATH, SAVE_PATH, test_size=0.1, valid_size=0.1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Corpus split into paired verse\n",
        "Corpus class to manage and tokenize the text corpus into training, validation, and testing sets for the LMST model. It processes the given corpus by splitting and encoding the text based on roles (challenger and defender) in battle rap verses, enabling these structured, tokenized data pairs to be used for the model training and evaluation."
      ],
      "metadata": {
        "id": "I8jLLKx_Yn8o"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OYdCLrsF0mCh",
        "outputId": "aee9a7de-2421-438d-be61-e7b9bd5181aa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bataille 1:\n",
            "Challenger: have fun tryna make a comeback have fun tryna have sex with a dick like a thumb tack prolly spend hey day bitten watching pornhub i am so hungry look for milfs like its i found your mom and she is my top hoe she kinda old but still gimme top tho she kinda fishy smell like a flounder but who cares i am still going to pound her call me lil clumsy because i am a klutz tryna fuck the pussy but put it in they butts then i pull out and have hey mom on suck my nuts do not come at me tryna be lyrical because if i have to i will go political build you up break you down like the fucking wall take you out by your knees i make hey bitch ass fall to the ground shut the fuck up ion want to here sound like honestly how can one be so fucking dumb maybe hey daddy pulled out and you was born off of premium anybody got brains because this bitch really needs sum\n",
            "Defender: hell you either know it or you own it i could give you some credit i understand you a little desperate i have lived their once but came back in sores rising from my grave with no remorse getting the sky ready with all something going to hit you hard i do not know what happened somethings in the dark haha wait thats me i hope your scared i will hit you down hard this am not thor shit i once lived in hell now i am on top at im not like thor he missed once but i have gone in for the kill i got plenty of skill enough to fulfill later bitch\n",
            "------------------------------\n",
            "Bataille 2:\n",
            "Challenger: he is everywhere writing those shit lines and losing weak lost every battle he is been in cause he is a joke his ego is more fat then godzilla egg yolk a hopeless fool who think he is on part i would measure his stupidity but i would not count the stars xxxxoponentxxxx said he would body but he was playing uno but he was getting cocky and got caught like his name was johnny everyone him and no one likes him and think that he was his childhood he is abuse like a child would his arms are spread out like christ is wood beaten to the ground and given no pounds i assassinate head on i beat into the bush not around only one sound his mouth noun he is crying help for someone to look take pity and review him like yelp cause i am sick of you tormenting us with your nonsensical words that is why your a victim of this purge on the verge on having to merge with a scourge id urge you to run but you have already in the radius it is no use to regret things now a few meaningless lines to be annexed to your nights this is just percent of me cause you have not even worth a tenth of this now stay down now now it is alright to be wordless after all you have just the clown i have claimed another soul now onto the entire circus\n",
            "Defender: it is not weak just weak you do not want this battle as your new admin ima have to postpone it this is lyrical surgery you could say i pro boned it your a stupid ass kid i know the rapped community people do not know who you are it is like your identity was stolen i was about someone deformed i could have sworn that incest was how you were born all this hail and i stay warm like i was a cold blooded life form who just turned this piece of shit to ashes your new name is dust storm\n",
            "------------------------------\n",
            "Bataille 3:\n",
            "Challenger: look like a maverick look like a savage on the dot see this glock i am who you say i am\n",
            "Defender: man you am not even got a chance you am not even advanced do not even try and battle me when you looking like a trans\n",
            "------------------------------\n",
            "Bataille 4:\n",
            "Challenger: unscathed verbally this is clash of the titans with massive blasting blazing infernally these giants i unleash from my salivary glands while you washing with my raps ravage ya while you dancing in zion you like a cat up against the lion abe smashing fakes i slashing faces with diamond shards squashing snakes hiding in bases leave them flattened like cards fuck what you claim for ages you not i hope you rotten in you not hotness you sweeter than frosting in cakes armed with plastic rakes forgotten in yards you not a monstrous creature commanding the lakes like the richness you losing wealth faggot my flows acidic on shows prolific like prose am over analytic you aint gifted from here lifted everything about you is mythic i crucify you of everyone like the critic jumping around like a stupid cricket gay enough to give cupid biscuit am too artistic here to abuse the misfit passing through the ticket using a multi ticket proven wicked till the end of class you can take my shoe i lick it id eva recommend your ass to a true linguistic scripting with black cracking a new hieroglyphic i hijack moons an exhale jus takin you too simplistic punch in lines like a statistical typist my cannibalistic burning the scene leave it lookin like dragon crimes ready to beat your million with a rhymes you pass whack weak lil minion with departed my thoughts like skies so dark god to throw a star at mines lyrically scarring mimes i finish this quicker than flames pines the ripper of lames with classic vines you fell in love but it was not an angel king sent from above now you sit on a pile of ashes with her shit scent on ya cove reminisce dissing with every thought ya mind be spilling leave you holier than sponge bob every line be drilling shoulda lock her ass inside a building now shes man wife with children working with a knife in the kitchen dressed like she wanted in a tight lil mitten an apple piece has been bitten now you got no friends brother stick to feeding the kitten a bleed on a written throw a free for free knock you out before counting to three you can chill to choke like judas hung on a tree branches covered with your lung i debris for tryna mess with a when you just a bee its always a massacre if a show on the spree people gettin hyped smoking degree the full law when i steadily type even the popo agree betrayed by a whore like got a fat bitch at home but prefer a maria ten like a jew and still have time to laugh tending to you could not keep a girl venting online cupid infected you through you like a rap infection too stupid to shoo fuckin rat have fun tryna live in a shoe am over offensive the despicable sue battle again you wo not regret eating the glue next time bring a goblin a witch ana crew i shit on ya stew make you watch the start of your body splitting in two experiment on your hemoglobin switch it to blue you lost on the rap game homie no no given a clue for now bye bi that is visibly new am so fly every lyric flew you so sly you secretly talk about some dick that you blew dont trust the traitorous bitches to prevail in riches not like this present where you wailing in ditches like a barbie thug crying blood degrading in you got to grab the bull by the horn to keep controlling the switches peace out before explode and you suffer the twitches\n",
            "Defender: yeah yeah any they say a whole lotta shit i do not want to hear so i had to turn up on them turn up if you hearing this song right here nah prolly did yeah i really turn up on you turn up on you young sui only why he tryna rape why he tryna rape yeah i took the age off it took the age off it i seen the vision yeah i took my chance i took my chance you pussy hit the brakes on them git git git i hit the gas like zoom i hit the gas walking around brand new lou is brand new brand new anytime i am in the room i am in the room they scream my name like su they scream my name i hit the gas like zoom i hit the gas walking around brand new lou is brand new brand new anytime i am in the room i am in the room they scream my name like su i say the shit they do not want to hear and i type it all caps type it all caps i do not even know and you say my brother but you switching up and that is facts know that that is facts gotta keep notes to the shit that he did so i know they never come back come back mamma told me not to hold on to but that is just a knife in my back knife in my back why he a knife in my back thought why he a knife bitch i am just a younging kicking trap doors git git crash out the whip give a fuck about a bitch ima smash on his head in the dashboard dashboard reason i rap about the shit rap about i either have seen it or been through it been through it all of the shit that goes down in the streets yeah they shooting and even the kids knew it yeah pops raised me to shoot first keep my foot on their neck so there am not no shooting back birth git git and i got like way too much to live for need them gone got crew for that i got a crew for that i got to thank god when i wake up for my life it is a movie man for my life i know they hate me cause i am winning and this am not nothing new to me new to me stuck in this daydream please save me from the way that i am moving yeah ai not gotten no sleep in the past week knew the music would do all this all of my day ones moving shady never plan on me losing them losing them but i cannot be weak family needs me need a duffel with blues in it git git i hit the gas like zoom walking around brand new lou is brand new brand new anytime i am in the room i am in the room they scream my name like su they scream my name i hit the gas like zoom i hit the gas walking around brand new lou is brand new brand new anytime i am in the room i am in the room they scream my name like su like sun\n",
            "------------------------------\n",
            "Bataille 5:\n",
            "Challenger: generic funeral intro as i sit in this church looking at your casket i feel the cold straight through my jacket i sigh as i look at our i never thought you would go into this mess i was there for you i told you that i said if you to i would be there for a chat you left me here when i you most i picked up your call and i heard your tears your voice breaking started to strengthen my fears i asked you why i did not hear what you said as you started to cry i heard the cock of the gun and your body hit the floor i drove over and kicked down your door i fell to my knees and cried on your shoulder it felt like my life just got crushed by a boulder whyd you have to leave you told me to believe we were going to make it big now look at what you did you left me on my own now i am looking at my phone wondering where i went wrong got me singing this song i did not think itd all depend but i hope that i will see you soon friend interlude place for yeah life gets cut short sometimes some of us handle it with rhymes we focus on the wrong things in our this life has gotten playin with knives drugs shorten our lives but we still take every drug we get our hands on driving through the town all i see is neon people driving pissant i do not need a response homie is over here smoking rocks got us all feeling like gods all i see are dots people taking shots talked to your pops said he missed you i do too you were a friend through and through we stuck together like glue but me being left is not new\n",
            "Defender: ma cherie forgive me for the pain i have you i beg for the very thing you always say you cannot do forgiveness for the way i never to you when you asked for me i for somethin different to do happily whistling tunes like skip to my lou till i pushed you into the tomb i understand if there am not no mending the broken heart after i ripped it in two i remember when you told me you would be my only one but you started mistrusting me that is when my woes begun i still remember the first time we met it was love at first sight but overtime we grew distant till we started to split that was the worst night we were younger back then the world was our oyster but i had to clam up and become this soulless destroyer my insecurities really worried me i told you i was made of steel i began to shut you out never to how you would say you feel i would tell myself this am not for real cause i am supposed to be in love but we been together a while sometimes it feels as though i have seen enough some nights i want to cuddle some nights i would rather drink cyanide i turn over in the bed and cannot tell if you still by my side so my love i hope this letter you in good health cause i am not rhyming about no girl i was rhyming to my self\n",
            "------------------------------\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "from collections import Counter\n",
        "\n",
        "class Corpus(object):\n",
        "    \"\"\"\n",
        "    A class to handle the tokenization of a text corpus for training, validation, and testing in NLP models.\n",
        "\n",
        "    Attributes:\n",
        "    tokenizer (EnhancedWordLevelTokenizer): An instance of tokenizer used to encode the text.\n",
        "    train (list): List of tokenized verse pairs (challenger and defender) for training.\n",
        "    valid (list): List of tokenized verse pairs for validation.\n",
        "    test (list): List of tokenized verse pairs for testing.\n",
        "\n",
        "    Methods:\n",
        "    tokenize(path): Tokenizes the verses from a file at the given path.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, path, tokenizer):\n",
        "        # Initialize the corpus with tokenized training, validation, and testing sets\n",
        "        self.tokenizer = tokenizer\n",
        "        self.train = self.tokenize(os.path.join(path, 'train.txt'))\n",
        "        self.valid = self.tokenize(os.path.join(path, 'valid.txt'))\n",
        "        self.test = self.tokenize(os.path.join(path, 'test.txt'))\n",
        "\n",
        "    def tokenize(self, path):\n",
        "        \"\"\"\n",
        "        Tokenizes a text file.\n",
        "\n",
        "        Args:\n",
        "        path (str): Path to the file to be tokenized.\n",
        "\n",
        "        Returns:\n",
        "        List of tokenized verse pairs (challenger, defender) in the file.\n",
        "\n",
        "        The function reads the file line by line, distinguishing between challenger and defender verses,\n",
        "        and tokenizes each verse using the provided tokenizer. Each battle is represented as a pair of tokenized verses.\n",
        "        \"\"\"\n",
        "        verse_pairs = []\n",
        "        challenger_verse, defender_verse = [], []\n",
        "        is_challenger = True\n",
        "\n",
        "        with open(path, 'r', encoding='utf-8') as f:\n",
        "            for line in f:\n",
        "                line = line.strip()\n",
        "                if line == \"<battle_start>\":\n",
        "                    challenger_verse, defender_verse = [], []\n",
        "                elif line == \"<challenger>\":\n",
        "                    is_challenger = True\n",
        "                elif line == \"<defender>\":\n",
        "                    is_challenger = False\n",
        "                elif line.startswith(\"<sos>\"):\n",
        "                    verse_content = line[5:].split(\"<eos>\")[0].strip()  # Extract verse content\n",
        "                    verse = self.tokenizer.encode(verse_content)\n",
        "                    if is_challenger:\n",
        "                        challenger_verse.extend(verse)\n",
        "                    else:\n",
        "                        defender_verse.extend(verse)\n",
        "                elif line == \"<battle_end>\":\n",
        "                    if challenger_verse and defender_verse:\n",
        "                        verse_pairs.append((torch.tensor(challenger_verse, dtype=torch.long),\n",
        "                                            torch.tensor(defender_verse, dtype=torch.long)))\n",
        "                    challenger_verse, defender_verse = [], []\n",
        "\n",
        "        return verse_pairs\n",
        "\n",
        "# Example usage of the Corpus class\n",
        "corpus = Corpus(SAVE_PATH, tokenizer)\n",
        "\n",
        "# Print verses from the challenger and defender for the first few battles\n",
        "num_battles_to_display = 5  # Number of battles to display\n",
        "for i, (challenger_verse, defender_verse) in enumerate(corpus.train):\n",
        "    if i >= num_battles_to_display:\n",
        "        break\n",
        "\n",
        "    challenger_text = ' '.join([tokenizer.index_word[idx] for idx in challenger_verse.numpy()])\n",
        "    defender_text = ' '.join([tokenizer.index_word[idx] for idx in defender_verse.numpy()])\n",
        "\n",
        "    print(f\"Bataille {i+1}:\")\n",
        "    print(\"Challenger:\", challenger_text)\n",
        "    print(\"Defender:\", defender_text)\n",
        "    print(\"-\" * 30)  # Separator for clarity\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Embedding & LSTM model - to run each time\n",
        "The provided script defines a neural network model for natural language processing tasks using Long Short-Term Memory (LSTM). The model features a hybrid embedding layer that integrates both pre-trained and trainable embeddings, enhancing its ability to handle a diverse vocabulary including out-of-vocabulary words. The LSTM layers in the model capture the temporal dependencies in the sequence data. This model is particularly suited for tasks involving sequential text data, like language modeling, text generation, or even more complex tasks such as sentiment analysis or machine translation. The inclusion of a hybrid embedding layer makes the model robust in handling a wide range of words, improving its overall performance on various natural language processing tasks."
      ],
      "metadata": {
        "id": "DZ81v8RaaiVJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3pfzx0II26o7"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "from gensim.models import KeyedVectors\n",
        "\n",
        "class HybridEmbeddingLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    A layer that combines trainable embeddings with pretrained embeddings.\n",
        "\n",
        "    Attributes:\n",
        "    embedding (torch.nn.Embedding): An embedding layer.\n",
        "\n",
        "    Methods:\n",
        "    forward(x): Forward pass for embedding lookup.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_embeddings, embedding_dim, pretrained_embeddings=None, oov_indices=None):\n",
        "        \"\"\"\n",
        "        Initialize the HybridEmbeddingLayer.\n",
        "\n",
        "        Args:\n",
        "        num_embeddings (int): The size of the vocabulary.\n",
        "        embedding_dim (int): The size of each embedding vector.\n",
        "        pretrained_embeddings (Tensor, optional): A tensor containing pretrained embeddings.\n",
        "        oov_indices (list, optional): Indices of out-of-vocabulary words.\n",
        "        \"\"\"\n",
        "        super(HybridEmbeddingLayer, self).__init__()\n",
        "        self.embedding = nn.Embedding(num_embeddings, embedding_dim)\n",
        "        if pretrained_embeddings is not None:\n",
        "            self.embedding.weight.data.copy_(pretrained_embeddings)\n",
        "            if oov_indices is not None:\n",
        "                self.embedding.weight.data[oov_indices].normal_()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.embedding(x)\n",
        "\n",
        "class LSTMModel(nn.Module):\n",
        "    \"\"\"\n",
        "    A LSTM based sequence model.\n",
        "\n",
        "    Attributes:\n",
        "    ntoken (int): Number of tokens in the vocabulary.\n",
        "    nhid (int): Number of hidden units per layer.\n",
        "    nlayers (int): Number of LSTM layers.\n",
        "    initrange (float): Range for weight initialization.\n",
        "    drop (torch.nn.Dropout): Dropout layer.\n",
        "    encoder (HybridEmbeddingLayer): Embedding layer.\n",
        "    rnn (torch.nn.LSTM): LSTM network.\n",
        "    decoder (torch.nn.Linear): Linear decoder layer.\n",
        "\n",
        "    Methods:\n",
        "    create_weight_matrix(ntoken, emb_dim, word2vec): Creates the weight matrix for embeddings.\n",
        "    init_weights(): Initializes weights.\n",
        "    init_hidden(bsz): Initializes hidden states.\n",
        "    forward(input, hidden, lengths): Forward pass of the model.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, ntoken, ninp, nhid, nlayers, dropout=0.2, initrange=0.1, word2vec_path=None):\n",
        "        \"\"\"\n",
        "        Initialize the LSTM model.\n",
        "\n",
        "        Args:\n",
        "        ntoken (int): Number of tokens in the vocabulary.\n",
        "        ninp (int): Size of the embeddings.\n",
        "        nhid (int): Number of hidden units per LSTM layer.\n",
        "        nlayers (int): Number of LSTM layers.\n",
        "        dropout (float): Dropout probability.\n",
        "        initrange (float): Range for weight initialization.\n",
        "        word2vec_path (str, optional): Path to pretrained Word2Vec embeddings.\n",
        "        \"\"\"\n",
        "        super(LSTMModel, self).__init__()\n",
        "        self.ntoken = ntoken\n",
        "        self.nhid = nhid\n",
        "        self.nlayers = nlayers\n",
        "        self.initrange = initrange\n",
        "\n",
        "        weights_matrix = None\n",
        "        oov_indices = []\n",
        "\n",
        "        if word2vec_path:\n",
        "            word2vec = KeyedVectors.load_word2vec_format(word2vec_path, binary=True)\n",
        "            weights_matrix, oov_indices = self.create_weight_matrix(ntoken, ninp, word2vec)\n",
        "\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        self.encoder = HybridEmbeddingLayer(ntoken, ninp, pretrained_embeddings=weights_matrix, oov_indices=oov_indices)\n",
        "        self.rnn = nn.LSTM(ninp, nhid, nlayers, dropout=(dropout if nlayers > 1 else 0))\n",
        "        self.decoder = nn.Linear(nhid, ntoken)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def create_weight_matrix(self, ntoken, emb_dim, word2vec):\n",
        "        \"\"\"\n",
        "        Creates a weight matrix for the embedding layer from a Word2Vec model.\n",
        "\n",
        "        Args:\n",
        "        ntoken (int): Number of tokens.\n",
        "        emb_dim (int): Embedding dimension.\n",
        "        word2vec (gensim.models.KeyedVectors): Pre-trained Word2Vec model.\n",
        "\n",
        "        Returns:\n",
        "        Tuple of weights_matrix (Tensor) and oov_indices (list).\n",
        "        \"\"\"\n",
        "        weights_matrix = torch.zeros(ntoken, emb_dim)\n",
        "        oov_indices = []\n",
        "        for i, word in enumerate(vocabulary):\n",
        "            try:\n",
        "                weights_matrix[i] = torch.from_numpy(word2vec[word])\n",
        "            except KeyError:\n",
        "                oov_indices.append(i)\n",
        "        return weights_matrix, oov_indices\n",
        "\n",
        "    def init_weights(self):\n",
        "        \"\"\"\n",
        "        Initialize weights of the model.\n",
        "        \"\"\"\n",
        "        nn.init.uniform_(self.encoder.embedding.weight, -self.initrange, self.initrange)\n",
        "        nn.init.zeros_(self.decoder.bias)\n",
        "        nn.init.uniform_(self.decoder.weight, -self.initrange, self.initrange)\n",
        "\n",
        "    def init_hidden(self, bsz):\n",
        "        \"\"\"\n",
        "        Initialize the hidden states of the LSTM.\n",
        "\n",
        "        Args:\n",
        "        bsz (int): Batch size.\n",
        "\n",
        "        Returns:\n",
        "        A tuple of tensors representing the initial hidden state and cell state.\n",
        "        \"\"\"\n",
        "        weight = next(self.parameters()).data\n",
        "        return (weight.new_zeros(self.nlayers, bsz, self.nhid),\n",
        "                weight.new_zeros(self.nlayers, bsz, self.nhid))\n",
        "\n",
        "    def forward(self, input, hidden, lengths):\n",
        "        \"\"\"\n",
        "        Forward pass of the LSTM model.\n",
        "\n",
        "        Args:\n",
        "        input (Tensor): Input tensor.\n",
        "        hidden (tuple): Initial hidden and cell states.\n",
        "        lengths (Tensor): The length of each sequence in the batch.\n",
        "\n",
        "        Returns:\n",
        "        Output tensor and the final hidden state.\n",
        "        \"\"\"\n",
        "        emb = self.drop(self.encoder(input))\n",
        "\n",
        "        packed_emb = pack_padded_sequence(emb, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
        "        packed_output, hidden = self.rnn(packed_emb, hidden)\n",
        "\n",
        "        max_length = input.size(1)\n",
        "        output, _ = pad_packed_sequence(packed_output, batch_first=True, total_length=max_length)\n",
        "\n",
        "        return output, hidden\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Batching & Padding - to run each time\n",
        "This script creates a custom Dataset and DataLoader for handling rap battle verses. The Dataset class PaddedBatchedRapBattles is designed to equalize the length of verses within each pair and sort them by length for efficient batching. It also includes a custom collation function to handle the padding and batching of variable-length sequences. This setup is particularly useful for tasks involving sequential data, such as language modeling or text generation, where input sequences can vary in length."
      ],
      "metadata": {
        "id": "438H4jGcb3Kn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xDP46e-2P-NI"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "class PaddedBatchedRapBattles(Dataset):\n",
        "    \"\"\"\n",
        "    A Dataset class for handling rap battle verse pairs, ensuring they are padded and batched efficiently.\n",
        "\n",
        "    Attributes:\n",
        "    pad_idx (int): Index used for padding.\n",
        "    sorted_verse_pairs (list): List of verse pairs, sorted by their length.\n",
        "\n",
        "    Methods:\n",
        "    equalize_verse_pairs_lengths(verse_pairs): Equalizes the length of verses in each pair.\n",
        "    sort_verse_pairs_by_length(verse_pairs): Sorts verse pairs by their total length.\n",
        "    __len__(): Returns the number of verse pairs.\n",
        "    __getitem__(idx): Retrieves a verse pair based on the index.\n",
        "    collate_fn(batch): Custom collate function to batch verses.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, verse_pairs, pad_idx):\n",
        "        \"\"\"\n",
        "        Initializes the dataset with verse pairs that are equalized in length and sorted.\n",
        "\n",
        "        Args:\n",
        "        verse_pairs (list of tuples): List of tuples where each tuple contains two verses (challenger and defender).\n",
        "        pad_idx (int): Index used for padding.\n",
        "        \"\"\"\n",
        "        self.pad_idx = pad_idx\n",
        "        equalized_verse_pairs = self.equalize_verse_pairs_lengths(verse_pairs)\n",
        "        self.sorted_verse_pairs = self.sort_verse_pairs_by_length(equalized_verse_pairs)\n",
        "\n",
        "    def equalize_verse_pairs_lengths(self, verse_pairs):\n",
        "        \"\"\"\n",
        "        Equalizes the lengths of verses in each verse pair by padding them.\n",
        "\n",
        "        Args:\n",
        "        verse_pairs (list of tuples): Verse pairs to be equalized.\n",
        "\n",
        "        Returns:\n",
        "        List of equalized verse pairs.\n",
        "        \"\"\"\n",
        "        equalized_verse_pairs = []\n",
        "        for challenger_verse, defender_verse in verse_pairs:\n",
        "            max_length = max(len(challenger_verse), len(defender_verse))\n",
        "            challenger_verse_padded = list(challenger_verse) + [self.pad_idx] * (max_length - len(challenger_verse))\n",
        "            defender_verse_padded = list(defender_verse) + [self.pad_idx] * (max_length - len(defender_verse))\n",
        "            equalized_verse_pairs.append((challenger_verse_padded, defender_verse_padded))\n",
        "        return equalized_verse_pairs\n",
        "\n",
        "    def sort_verse_pairs_by_length(self, verse_pairs):\n",
        "        \"\"\"\n",
        "        Sorts verse pairs by the total length of their verses.\n",
        "\n",
        "        Args:\n",
        "        verse_pairs (list): Verse pairs to be sorted.\n",
        "\n",
        "        Returns:\n",
        "        Sorted list of verse pairs.\n",
        "        \"\"\"\n",
        "        return sorted(verse_pairs, key=lambda x: len(x[0]) + len(x[1]))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sorted_verse_pairs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        challenger_verse, defender_verse = self.sorted_verse_pairs[idx]\n",
        "        return torch.tensor(challenger_verse, dtype=torch.long), torch.tensor(defender_verse, dtype=torch.long)\n",
        "\n",
        "    def collate_fn(self, batch):\n",
        "        \"\"\"\n",
        "        Custom collation function to group verses in a batch, handling variable-length sequences.\n",
        "\n",
        "        Args:\n",
        "        batch (list): Batch of verse pairs.\n",
        "\n",
        "        Returns:\n",
        "        Tuple of tensors representing padded challenger and defender verses.\n",
        "        \"\"\"\n",
        "        challenger_batch, defender_batch = zip(*batch)\n",
        "        challenger_padded = pad_sequence([v.clone().detach() for v in challenger_batch], batch_first=True, padding_value=self.pad_idx)\n",
        "        defender_padded = pad_sequence([v.clone().detach() for v in defender_batch], batch_first=True, padding_value=self.pad_idx)\n",
        "        return challenger_padded, defender_padded\n",
        "\n",
        "def create_dataloader(verse_pairs, batch_size, pad_idx):\n",
        "    \"\"\"\n",
        "    Creates a DataLoader for the rap battle verses dataset.\n",
        "\n",
        "    Args:\n",
        "    verse_pairs (list): List of verse pairs.\n",
        "    batch_size (int): Size of each batch.\n",
        "    pad_idx (int): Padding index.\n",
        "\n",
        "    Returns:\n",
        "    DataLoader object.\n",
        "    \"\"\"\n",
        "    dataset = PaddedBatchedRapBattles(verse_pairs, pad_idx)\n",
        "    return DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=dataset.collate_fn)\n",
        "\n",
        "# `pad_idx` define\n",
        "PAD_IDX = tokenizer.word_index[\"<pad>\"]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Batching & Padding - Test - No need run each time\n",
        "This script demonstrates how to use the PaddedBatchedRapBattles dataset and create_dataloader function to create a DataLoader for training. It initializes the dataset with training data, specifying the padding index to handle variable-length sequences. A DataLoader is then created with a specified batch size. The script iterates through the first two batches of the DataLoader, displaying the contents of each batch for both challenger and defender verses. This setup is essential for training models on sequential data, where efficient batching and padding are crucial for handling varying sequence lengths."
      ],
      "metadata": {
        "id": "CWGflaGKc9rX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eAXfL9m3clWA",
        "outputId": "0915a07b-a5b6-4489-c8a7-a2e82f2ad85e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 1 :\n",
            "Challenger Batch: tensor([[   79,    74,    13,   477,    28,    20,    17,    89,    38,     9,\n",
            "           125,    13,    30,    47,   143,    10,    39,    27,    11,   536,\n",
            "           244,   487,    10,    13,    17,  5301,  3878,    14,     9,    16,\n",
            "            65,    13,   259,    17,   170,    13,    12, 13992,   104,    20,\n",
            "            11,  1953,  2014,    49,    10,    65,    13,   221,   471,    84,\n",
            "            22,    13,    18,   148,    49,    10,    30,    19,     9,    16,\n",
            "            35,   584,    63,    80,    17,   302,   104,   389,    58,    11,\n",
            "          4658,    75,   122,    12,  2865,     9,    37,    20,    18,   636,\n",
            "             9,    30,    19,   189,   438,  2014,  2014,  2014,  1495,     9,\n",
            "            34,  1670,    21,    79,    18,  2834,     9,   635,    24,    45,\n",
            "            30,    19,    34,  1799,    23,    41,   260,     9,    31,   477,\n",
            "            28,    60,     9,    16,    12,  6708,    42,     9,    16,    39,\n",
            "            28,    11,   902,     9,    21,    13,    63,   137,    81,   133,\n",
            "           147,     9,   140,    21,    12,   351,  5508],\n",
            "        [   10,  2651,     9,    57,    17,   674,    20,   152,    88,    47,\n",
            "          2788,    59,    17,   674,   322,    18,   160,   114,    56,    10,\n",
            "           607,    46,  1456,     2,     2,     2,     2,     2,     2,     2,\n",
            "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "             2,     2,     2,     2,     2,     2,     2]])\n",
            "Defender Batch: tensor([[  147,    42,   126,   484,    14,    70,    45,  1592,    39,    12,\n",
            "          4332,    13,   562,  2188,    13,   185,  1014,    45,  3388,    20,\n",
            "            21,    12,  1418,  1921,    29,    10,    37,    13,   174,  4401,\n",
            "            20,    28,    11,  2621,    72,    11,   597,    17,  5014,   203,\n",
            "           307,   134, 26559,   134,    11,  3630,     2,     2,     2,     2,\n",
            "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "             2,     2,     2,     2,     2,     2,     2],\n",
            "        [   10,    84,    10,    64,    34,    18,  2737,    41,   119,    10,\n",
            "            35,    12,   278,    32,    30,    19,   116,    36,    18,  1376,\n",
            "            60,     9,    52,    30,   108,    95,    10,    10,    47,    75,\n",
            "             9,  1706,  1634,     2,     2,     2,     2,     2,     2,     2,\n",
            "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "             2,     2,     2,     2,     2,     2,     2]])\n",
            "----------------------------------------\n",
            "Batch 2 :\n",
            "Challenger Batch: tensor([[   10,   106,  5159,    13,    40,    17,  5823,   938,    22,    99,\n",
            "           294,    22,    15,  1036,   288,    10,    33,  6438,    27,    83,\n",
            "          1210,  1651,    62,  1131,    15,    21,    54,    15,  4481,     9,\n",
            "            33,   396,    85,  1839,    21,  1819,  8672,     9,    16,   108,\n",
            "            11,   291,  5787,  4007,    22,    15,  3214,    25,   499,    14,\n",
            "            10,    33,  3948,    11,   815,   464,    18,   503,    15,   551,\n",
            "           252,    10,    38,   105,   130,   621,    22,    11,   567,   543,\n",
            "             9, 12008,    20,    12,  1515,   143,    62,   391,    10,   352,\n",
            "           496,   197,    27,  1622,  3622,    21,    10,   352,   496,   197,\n",
            "            27,   272, 12333,   543,     9,    33,  2997, 11476,    14,  7057,\n",
            "            12,  3939,    29,    11,  1812,    32, 10129,    11,  3172,  5470,\n",
            "            36,    12,  1268,    25,  4334,    17,   204,    15, 11778,   551,\n",
            "            25,    38,    11,   108,    10,    33,    19,    37,    11,   938,\n",
            "            13,   108,    23,    22,    15,    12,  3451,    25,  4353,  4943,\n",
            "           149,    13,  1060,  1755,    11,  4424,   342,    83,     9,  2890,\n",
            "           815,    25,  5191,  1736,    28,    38,  4100,    29,    38,  6438,\n",
            "         14374,    21,    11,  3312,    25,  9161,    29,    11,   623,    25,\n",
            "            83,   202,    15,   149,    13,  2160,  1739,    83,   397,  2052,\n",
            "            21,  2110,    29,  1702,    26,  2194,    55,    12,   745,    15,\n",
            "          1458,    17,   503,    15,  1384,    18,  5096,  2319,    62,    23,\n",
            "          1167,   430,   350,    54,    15,    71,   430,   298,   130,    32,\n",
            "           149,    13,   326,    90,    83,  3794,  3372,  1977,   328,    53,\n",
            "            18,   422,    15,    12,  2154,  3602,     9,   196,   319,    14,\n",
            "         17830,    46,    12,   171,    27,  1519,    26,    15,   480,    14,\n",
            "           418,   124,   312,    21, 15311,    20,  1659,    22,    15,    19,\n",
            "           128,    13,  3315,    10,    33,   276,  1102,    14,   233,   215,\n",
            "           127,   622,   561,    21,    12,  1554,    10,    33,    95,    12,\n",
            "         16458,  3083,    14,  9120,    78,   911,  4234, 18820,    29,   114,\n",
            "            14,    62,    83,  1999,   202,   113,    34,   128,   999,    11,\n",
            "            39,   661,  9467, 11303,     9,   130,    12,   420,    98,    72,\n",
            "          8365,    70,     9,    33,    37,   155,    14, 12888,    12,  2563,\n",
            "            15,  7462,   455,     9,    16,  1250,    55,    12,   536,    49,\n",
            "            54,    15,    54,    31,   406,    24,   535,    21,    12,  9542,\n",
            "            13,    11,   638,    41, 14711,  1724,   204,    32,    49,    54,\n",
            "            15,   396,   541,   269,     9,   130,     9,    31,     9,  2972,\n",
            "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "             2,     2,     2,     2],\n",
            "        [  713,    15,   106,    91,    22,    13,    34,   184,  2595,    77,\n",
            "             9,    16,    32,   418,    58,   153,     9,   398,    13,    34,\n",
            "            56,    26,   535,  3274,    78,    50,    86,     9,   398,    13,\n",
            "            34,    12,  1135,    10,    13,    40,   252,   121,    11,  4647,\n",
            "           233,    17,   785,  4116,    12,  5069,  2765,    10,    30,    19,\n",
            "            40,  3489,   106,    12,  3926,   145,    10, 10435,   107,    15,\n",
            "            41,  3452,   382,   120,   515,    35,    21,    11,   642,    22,\n",
            "            15,    38,    20,   120,   132,    11,  3452,    43,    19,    20,\n",
            "            17,   257,    11,  1072,    15,    20,   120,   232,    24,    15,\n",
            "           109,    22,  1212,    20,    12,  1267,   126,  8242,    35,    13,\n",
            "          7176,    38,    11,   260,    45,    40,    32,   236,    55,   236,\n",
            "            55,    11,   436,  1432,    26,    12,   533,  2009,    27,   753,\n",
            "          2274,  7177,    33,    12,   402,    14,   107,    15,    12,   613,\n",
            "            36,    11,   126,   693,    45,    43,   327,    29,    96,   145,\n",
            "           114,    20,    11,  1183,   327,    15,    32,  2444,     9,   381,\n",
            "            22,    47,  5323,  4565,    15,    12,  4381,   141,    15,    12,\n",
            "          1849,  2798,    14,   840,    15,    11,   172,   415,   380,    15,\n",
            "            37,   210,    13,  1027,    37,   210,    13,   369,    45,    30,\n",
            "            19,   316,    66,    11,    72,    78,    11,  3170,    14,    45,\n",
            "           316,    78,   604,    36,    38,    11,  1803,    10,    63,   134,\n",
            "            45,    31,   145,   330,    11,  3610,    35,    36,    12,   158,\n",
            "           462,    25,  2571,    29,    26,    15,    11,   172,   190,    35,\n",
            "           429,    11,  1068,  5053,    46,  1517,  1029,   136,    12,  2229,\n",
            "            27,   120,   172,  2551,  1643,   125,    12,   535,    26,   172,\n",
            "           190,    15,   106,   205,   101,    11,  1087,  2067,    26,    15,\n",
            "           210,    51,    38,    37,    13,    40,   398,    13,    60,    26,\n",
            "            15,   210,    24,    31,    75,  4097,     2,     2,     2,     2,\n",
            "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "             2,     2,     2,     2]])\n",
            "Defender Batch: tensor([[   30,    19,   446,    42,    10,   368,    27,    23,     9,   141,\n",
            "            22,    42,    10,    33,  1630,    40,  4636,    42,    10,    33,\n",
            "          3515,    10,    31,   776,  1888,    42,    10,    33,  1995,   193,\n",
            "           199,   141,    42,    10,    33,  1588,    40,   589,    42,    10,\n",
            "          1559,    22,     9,  2332,    11,   736,    22,    15,   111,   108,\n",
            "          1800,    35,  4726,    22,  1800,    29,    34,  4618,    30,    19,\n",
            "           445,    11,  1835,    40,  4099,     9,    87,     9,   140,   199,\n",
            "           108,    42,    92,  8963,    13,    34,  7408,    15,  1048,    50,\n",
            "          2093,  2546,   193,    81,   141,  1340,    78,   171,  1764,    12,\n",
            "         13525,  9010,  2510,     9,   113,   205,    20,    17,  1968,    36,\n",
            "            12,   801,   404,  2514,    10,   113,   205,    20,    28,   497,\n",
            "          1968,    36,    12,   766,   404,   171,  9239,    11,  2364,    16,\n",
            "           105,    23,  1173,    22,    27,    19,    10,   189,   135,    13,\n",
            "            34,    21,    23,    10,    31,    97,    12,  3430,    36,    24,\n",
            "           951,    18,    15,   465,    16,    19, 18077,    41,   581,    36,\n",
            "           781,   147,    10,    52,    84,    16,    81,  7833,    24,    15,\n",
            "           109,     9,    16,   766,     9,    30,    19,    64,    13, 11220,\n",
            "            29,    16,    81,   801,   549,     9,    40,  4395,    24,    15,\n",
            "            42,     9,   824,    20,  3539,  1759,    81,    32,  3274,    40,\n",
            "           110,    23,   423, 17979, 10680,    11,   736,   900,  1983,    22,\n",
            "           152,    23,    91,   310,   736,    10,    52,   436,     9,    33,\n",
            "            37,   868,  1075,  1759,    12,  2248,    22,    15,    11,   423,\n",
            "         20695,   497,   423, 14760,    19,    12,  6200,     9,    30,    19,\n",
            "           145,    97,    17,   775,    57,     9,  1343,    28,    11,   183,\n",
            "             9,   245,  6489,    35,    13,    61,    12,  1871,    25,  4099,\n",
            "            40, 11199,    42,    10,   824,   501,    11,  5331,   125,   108,\n",
            "         15702,    16,   319,    29,  1044,    14,  2086,    16,  1588,  2588,\n",
            "            23,    11, 29192,   221,   658,    81,  3355,    10,   192,    97,\n",
            "            13,   673,    11,    44,    81,   282,   161,   108,   215,    17,\n",
            "            67,    42,    10,    33,  8208,    29,    30,    19,    10,   189,\n",
            "           117,    39,    28,   313,   147,    40,  8997,     9,    16,    35,\n",
            "           181,    13,    84,    16,  4018,  1339,    12,   394,    25, 15625,\n",
            "            24,    15,   109,    76,   214,    75,   100,    37,     9,   125,\n",
            "            13,    40,   588,    35,  1745,   183,     9,    87,     9,    21,\n",
            "            13,   337,    11,   901,   182,   147,     9,   141,   229,  1746,\n",
            "          3388,     9,    61,    81,  1483,     9,   145,   245,    13,   889,\n",
            "           147,    16,    46,    15,  4018,     9,   141,    22,    56,    11,\n",
            "          8766,    40,  1665,     9],\n",
            "        [    9,   111,   227,    36,    12,  1039,    53,    29,     9,    16,\n",
            "            67,    32,    59,   899,    26,    18,   116,    67,   820,    59,\n",
            "           205,    22,    67,   259,   182,    30,    12,  1325,  2190,  1087,\n",
            "             9,    16,    12,  2661,   188,    12,   392,   102, 14256,    20,\n",
            "            18,  1177,   102,    67,    20,    26,    59,    21,     9,    75,\n",
            "           206,   375,    36,    81,    44,    24,    10,    75,  1194,     9,\n",
            "            37,   589,    20,    12,  1127,  4103,   106,   235,    51,    50,\n",
            "            36,   365,    15,    13,   800,    11,   449,     2,     2,     2,\n",
            "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "             2,     2,     2,     2]])\n",
            "----------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Example of creating the dataset with padding and batching\n",
        "# Initialize the dataset using the training data and padding index\n",
        "train_dataset_example = PaddedBatchedRapBattles(corpus.train, PAD_IDX)\n",
        "\n",
        "# Use the create_dataloader function to create a DataLoader\n",
        "# You can adjust the batch size as per your requirements\n",
        "batch_size_example = 2\n",
        "train_loader_example = create_dataloader(corpus.train, batch_size_example, PAD_IDX)\n",
        "\n",
        "# Displaying details of the batches\n",
        "# Iterate through the DataLoader to access batches\n",
        "for i, batch in enumerate(train_loader_example):\n",
        "    if i >= 2:  # Limit to 2 batches for demonstration purposes\n",
        "        break\n",
        "\n",
        "    # Extract challenger and defender verses from the batch\n",
        "    challenger_batch, defender_batch = batch\n",
        "    print(f\"Batch {i + 1} :\")\n",
        "    print(\"Challenger Batch:\", challenger_batch)\n",
        "    print(\"Defender Batch:\", defender_batch)\n",
        "    print(\"-\" * 40)  # Separator for clarity\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Batching & Padding - on all corpus - to run each time\n",
        "This script sets up the DataLoaders for training, validation, and testing phases in a machine learning workflow. It first defines appropriate batch sizes for training and evaluation. Then, it initializes datasets for each phase (training, validation, testing) using the PaddedBatchedRapBattles class, which ensures that the data is consistently padded and batched. This class is particularly useful for sequential data like text, where inputs can vary in length. Finally, the script creates DataLoaders for these datasets, which will allow for efficient iteration over the data during model training and evaluation. The DataLoader for training uses a larger batch size compared to the evaluation DataLoader, reflecting common practice in machine learning to use smaller batches for evaluation to reduce memory requirements and potentially increase validation/test accuracy."
      ],
      "metadata": {
        "id": "H81rNeOhfeiw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gdnecB4eZSGh"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Define batch sizes for training and evaluation\n",
        "BATCH_SIZE = 8  # Number of samples per batch during training\n",
        "EVAL_BATCH_SIZE = 4  # Number of samples per batch during evaluation\n",
        "\n",
        "# Initialize datasets for training, validation, and testing\n",
        "# Using the PaddedBatchedRapBattles class ensures consistent padding and batch processing\n",
        "train_dataset = PaddedBatchedRapBattles(corpus.train, PAD_IDX)\n",
        "val_dataset = PaddedBatchedRapBattles(corpus.valid, PAD_IDX)\n",
        "test_dataset = PaddedBatchedRapBattles(corpus.test, PAD_IDX)\n",
        "\n",
        "# Create DataLoaders for the datasets\n",
        "# DataLoaders provide an efficient way to iterate over the datasets in batches\n",
        "# The create_dataloader function is used to create these loaders with specified batch sizes\n",
        "train_loader = create_dataloader(corpus.train, BATCH_SIZE, PAD_IDX)\n",
        "val_loader = create_dataloader(corpus.valid, EVAL_BATCH_SIZE, PAD_IDX)\n",
        "test_loader = create_dataloader(corpus.test, EVAL_BATCH_SIZE, PAD_IDX)\n",
        "\n",
        "# Now, train_loader, val_loader, and test_loader can be used in the training and evaluation loop\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train - Validate - Evaluate functions - to run each time\n",
        "This code provides the functions train, validate, and evaluate for an LSTM model. Each function performs a key step in the machine learning workflow: training the model with backpropagation, validating it on a separate dataset, and evaluating its performance on test data. The functions handle data from a DataLoader, which supplies inputs for the challenger and defender in a rap battle scenario. The model is trained on the challenger's verses first, with the hidden state then passed to the defender's verses, simulating a conversational context. Loss is computed based on the defender's output, reflecting the responsive nature of the training scenario. The model is optimized to predict the defender's response based on the challenger's input, considering the flow of the conversation."
      ],
      "metadata": {
        "id": "yMJ9ZLVehy8m"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iHeKV5XYj0II"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from tqdm import tqdm\n",
        "\n",
        "def train(model, train_loader, criterion, optimizer, device, pad_idx):\n",
        "    \"\"\"\n",
        "    Train the model for one epoch.\n",
        "\n",
        "    Args:\n",
        "        model: The LSTM model to be trained.\n",
        "        train_loader: DataLoader for the training data.\n",
        "        criterion: Loss function.\n",
        "        optimizer: Optimization algorithm.\n",
        "        device: Device to train on (e.g., 'cuda' or 'cpu').\n",
        "        pad_idx: Index of the padding token in the vocabulary.\n",
        "\n",
        "    Returns:\n",
        "        Average training loss for the epoch.\n",
        "    \"\"\"\n",
        "    model.train()  # Set the model to training mode\n",
        "    total_train_loss = 0\n",
        "\n",
        "    for batch_num, (challenger_inputs, defender_inputs) in enumerate(tqdm(train_loader, desc=\"Training\")):\n",
        "        # Transfer input data to the specified device (e.g., GPU)\n",
        "        challenger_inputs, defender_inputs = challenger_inputs.to(device), defender_inputs.to(device)\n",
        "\n",
        "        # Clear previous gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Initialize the hidden state of the LSTM\n",
        "        hidden = model.init_hidden(challenger_inputs.size(0))\n",
        "\n",
        "        # Forward pass through the model for the challenger\n",
        "        _, hidden = model(challenger_inputs, hidden, challenger_inputs.ne(pad_idx).sum(1))\n",
        "\n",
        "        # Forward pass for the defender, using the updated hidden state from the challenger's pass\n",
        "        output, _ = model(defender_inputs, hidden, defender_inputs.ne(pad_idx).sum(1))\n",
        "\n",
        "        # Decode the LSTM output to token space\n",
        "        output = model.decoder(output)\n",
        "\n",
        "        # Flatten the output and targets for loss computation\n",
        "        targets_flat = defender_inputs.view(-1)\n",
        "        output_flat = output.view(-1, model.ntoken)\n",
        "\n",
        "        # Calculate loss, ignoring the padding\n",
        "        loss = criterion(output_flat, targets_flat)\n",
        "        loss = loss.masked_select(targets_flat.ne(pad_idx)).mean()\n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "        # Perform backpropagation and optimize the model\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    avg_train_loss = total_train_loss / len(train_loader)\n",
        "    return avg_train_loss\n",
        "\n",
        "def validate(model, val_loader, criterion, device, pad_idx):\n",
        "    \"\"\"\n",
        "    Validate the model on the validation dataset.\n",
        "\n",
        "    Args:\n",
        "        model: The LSTM model to be validated.\n",
        "        val_loader: DataLoader for the validation data.\n",
        "        criterion: Loss function.\n",
        "        device: Device for validation (e.g., 'cuda' or 'cpu').\n",
        "        pad_idx: Index of the padding token in the vocabulary.\n",
        "\n",
        "    Returns:\n",
        "        Average validation loss.\n",
        "    \"\"\"\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    total_val_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for val_batch_num, (challenger_inputs, defender_inputs) in enumerate(tqdm(val_loader, desc=\"Validation\")):\n",
        "            challenger_inputs, defender_inputs = challenger_inputs.to(device), defender_inputs.to(device)\n",
        "            hidden = model.init_hidden(challenger_inputs.size(0))\n",
        "            _, hidden = model(challenger_inputs, hidden, challenger_inputs.ne(pad_idx).sum(1))\n",
        "            output, _ = model(defender_inputs, hidden, defender_inputs.ne(pad_idx).sum(1))\n",
        "            output = model.decoder(output)\n",
        "            targets_flat = defender_inputs.view(-1)\n",
        "            output_flat = output.view(-1, model.ntoken)\n",
        "            loss = criterion(output_flat, targets_flat)\n",
        "            loss = loss.masked_select(targets_flat.ne(pad_idx)).mean()\n",
        "            total_val_loss += loss.item()\n",
        "\n",
        "    avg_val_loss = total_val_loss / len(val_loader)\n",
        "    return avg_val_loss\n",
        "\n",
        "def evaluate(model, eval_loader, criterion, device, pad_idx):\n",
        "    \"\"\"\n",
        "    Evaluate the model on the test dataset.\n",
        "\n",
        "    Args:\n",
        "        model: The LSTM model to be evaluated.\n",
        "        eval_loader: DataLoader for the test data.\n",
        "        criterion: Loss function.\n",
        "        device: Device for evaluation (e.g., 'cuda' or 'cpu').\n",
        "        pad_idx: Index of the padding token in the vocabulary.\n",
        "\n",
        "    Returns:\n",
        "        Average loss on the test data.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_num, (challenger_inputs, defender_inputs) in enumerate(tqdm(eval_loader, desc=\"Evaluating\")):\n",
        "            challenger_inputs, defender_inputs = challenger_inputs.to(device), defender_inputs.to(device)\n",
        "            hidden = model.init_hidden(challenger_inputs.size(0))\n",
        "            _, hidden = model(challenger_inputs, hidden, challenger_inputs.ne(pad_idx).sum(1))\n",
        "            output, _ = model(defender_inputs, hidden, defender_inputs.ne(pad_idx).sum(1))\n",
        "            output = model.decoder(output)\n",
        "            targets_flat = defender_inputs.view(-1)\n",
        "            output_flat = output.view(-1, model.ntoken)\n",
        "            loss = criterion(output_flat, targets_flat)\n",
        "            loss = loss.masked_select(targets_flat.ne(pad_idx)).mean()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        avg_loss = total_loss / len(eval_loader)\n",
        "    return avg_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hyper-parameters setup - to run each time"
      ],
      "metadata": {
        "id": "mZOtXDA3i2lh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d4F0KTxwVnD6"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import time\n",
        "from gensim.models import KeyedVectors\n",
        "\n",
        "# Define constants and setup for the LSTM model training\n",
        "EMBEDDING_SIZE = 400  # Size of word embeddings\n",
        "HIDDEN_SIZE = 512     # Number of features in the hidden state of the LSTM\n",
        "N_LAYERS = 3          # Number of stacked LSTM layers\n",
        "DROPOUT = 0.3         # Dropout rate for regularization\n",
        "EPOCHS = 20           # Number of training epochs\n",
        "LEARNING_RATE = 0.01  # Learning rate for the optimizer\n",
        "WEIGHT_DECAY = 5e-4   # Weight decay (L2 penalty) for regularization\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # Training device\n",
        "\n",
        "# Load pre-trained Word2Vec embeddings\n",
        "word2vec = KeyedVectors.load_word2vec_format(WORD2VEC_PATH, binary=True)\n",
        "\n",
        "# Map words to indices for the model\n",
        "word_index = tokenizer.word_index  # Assume tokenizer is already initialized\n",
        "N_TOKENS = len(word_index)         # Total number of unique tokens in the vocabulary\n",
        "\n",
        "# Initialize the LSTM model with the specified parameters\n",
        "model = LSTMModel(\n",
        "    N_TOKENS, EMBEDDING_SIZE, HIDDEN_SIZE, N_LAYERS,\n",
        "    dropout=DROPOUT, word2vec_path=WORD2VEC_PATH\n",
        ").to(device)\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss(reduction='none')\n",
        "optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
        "\n",
        "# Learning rate scheduler to reduce the learning rate when a metric has stopped improving\n",
        "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, verbose=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training of the LSTM model - to run for each new training\n",
        "This script outlines the training and evaluation loop for an LSTM model. It includes training and validation phases for each epoch, adjusts the learning rate based on validation loss, and logs key metrics like epoch duration, learning rate, and losses. The script saves the best model based on validation loss and the final model after all epochs. Additionally, it performs quick evaluations on the test data after each epoch and a final evaluation at the end.\n",
        "\n",
        "This script includes early stopping. The training will stop if there's no improvement in the validation loss for a number of epochs equal to patience. After early stopping or completion of all epochs, the best model (based on validation loss) is loaded for final evaluation on the test data"
      ],
      "metadata": {
        "id": "-JjEF74XjY7b"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "8n-oKZS2nAe8",
        "outputId": "fc400e90-1f16-4423-b4b4-25694fe6de42"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1653/1653 [08:08<00:00,  3.38it/s]\n",
            "Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 368/368 [00:21<00:00, 17.52it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1, Time: 509.64s, LR: 0.001000, Training Loss: 3.6181, Validation Loss: 2.0465\n",
            "New best model saved to /content/drive/MyDrive/Colab Notebooks/save_files/best_model8.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 408/408 [00:21<00:00, 18.80it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Quick Test Loss: 1.9797\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1653/1653 [08:11<00:00,  3.36it/s]\n",
            "Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 368/368 [00:20<00:00, 17.56it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 2, Time: 512.89s, LR: 0.001000, Training Loss: 1.5689, Validation Loss: 1.1785\n",
            "New best model saved to /content/drive/MyDrive/Colab Notebooks/save_files/best_model8.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 408/408 [00:21<00:00, 18.83it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Quick Test Loss: 1.1188\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1653/1653 [08:09<00:00,  3.38it/s]\n",
            "Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 368/368 [00:21<00:00, 17.47it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 3, Time: 510.66s, LR: 0.001000, Training Loss: 0.8714, Validation Loss: 0.6967\n",
            "New best model saved to /content/drive/MyDrive/Colab Notebooks/save_files/best_model8.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 408/408 [00:21<00:00, 18.85it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Quick Test Loss: 0.6520\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1653/1653 [08:09<00:00,  3.37it/s]\n",
            "Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 368/368 [00:20<00:00, 17.53it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 4, Time: 510.81s, LR: 0.001000, Training Loss: 0.4753, Validation Loss: 0.4293\n",
            "New best model saved to /content/drive/MyDrive/Colab Notebooks/save_files/best_model8.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 408/408 [00:21<00:00, 18.92it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Quick Test Loss: 0.3972\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1653/1653 [08:10<00:00,  3.37it/s]\n",
            "Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 368/368 [00:20<00:00, 17.63it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 5, Time: 511.42s, LR: 0.001000, Training Loss: 0.2690, Validation Loss: 0.3049\n",
            "New best model saved to /content/drive/MyDrive/Colab Notebooks/save_files/best_model8.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 408/408 [00:21<00:00, 18.81it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Quick Test Loss: 0.2818\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1653/1653 [08:08<00:00,  3.38it/s]\n",
            "Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 368/368 [00:20<00:00, 17.54it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 6, Time: 509.90s, LR: 0.001000, Training Loss: 0.1513, Validation Loss: 0.2433\n",
            "New best model saved to /content/drive/MyDrive/Colab Notebooks/save_files/best_model8.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 408/408 [00:21<00:00, 18.86it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Quick Test Loss: 0.2218\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1653/1653 [08:09<00:00,  3.38it/s]\n",
            "Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 368/368 [00:21<00:00, 17.51it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 7, Time: 510.65s, LR: 0.001000, Training Loss: 0.0904, Validation Loss: 0.2070\n",
            "New best model saved to /content/drive/MyDrive/Colab Notebooks/save_files/best_model8.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 408/408 [00:21<00:00, 19.01it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Quick Test Loss: 0.1885\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1653/1653 [08:09<00:00,  3.38it/s]\n",
            "Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 368/368 [00:20<00:00, 17.72it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 8, Time: 510.40s, LR: 0.001000, Training Loss: 0.0636, Validation Loss: 0.2309\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 408/408 [00:21<00:00, 18.96it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Quick Test Loss: 0.2155\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1653/1653 [08:08<00:00,  3.38it/s]\n",
            "Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 368/368 [00:20<00:00, 17.57it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 9, Time: 509.45s, LR: 0.001000, Training Loss: 0.0753, Validation Loss: 0.1873\n",
            "New best model saved to /content/drive/MyDrive/Colab Notebooks/save_files/best_model8.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 408/408 [00:21<00:00, 19.03it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Quick Test Loss: 0.1745\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1653/1653 [08:09<00:00,  3.38it/s]\n",
            "Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 368/368 [00:21<00:00, 17.50it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 10, Time: 510.64s, LR: 0.001000, Training Loss: 0.0388, Validation Loss: 0.1765\n",
            "New best model saved to /content/drive/MyDrive/Colab Notebooks/save_files/best_model8.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 408/408 [00:21<00:00, 18.77it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Quick Test Loss: 0.1641\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1653/1653 [08:07<00:00,  3.39it/s]\n",
            "Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 368/368 [00:20<00:00, 17.69it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 11, Time: 508.77s, LR: 0.001000, Training Loss: 0.0257, Validation Loss: 0.1630\n",
            "New best model saved to /content/drive/MyDrive/Colab Notebooks/save_files/best_model8.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 408/408 [00:21<00:00, 18.92it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Quick Test Loss: 0.1530\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1653/1653 [08:07<00:00,  3.39it/s]\n",
            "Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 368/368 [00:21<00:00, 17.50it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 12, Time: 508.85s, LR: 0.001000, Training Loss: 0.0188, Validation Loss: 0.1577\n",
            "New best model saved to /content/drive/MyDrive/Colab Notebooks/save_files/best_model8.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 408/408 [00:21<00:00, 19.06it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Quick Test Loss: 0.1482\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1653/1653 [08:08<00:00,  3.39it/s]\n",
            "Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 368/368 [00:20<00:00, 17.54it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 13, Time: 509.03s, LR: 0.001000, Training Loss: 0.0139, Validation Loss: 0.1560\n",
            "New best model saved to /content/drive/MyDrive/Colab Notebooks/save_files/best_model8.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 408/408 [00:21<00:00, 18.86it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Quick Test Loss: 0.1459\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1653/1653 [08:09<00:00,  3.38it/s]\n",
            "Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 368/368 [00:21<00:00, 17.45it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 14, Time: 510.17s, LR: 0.001000, Training Loss: 0.0110, Validation Loss: 0.1546\n",
            "New best model saved to /content/drive/MyDrive/Colab Notebooks/save_files/best_model8.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 408/408 [00:21<00:00, 18.82it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Quick Test Loss: 0.1431\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1653/1653 [08:08<00:00,  3.38it/s]\n",
            "Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 368/368 [00:20<00:00, 17.55it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 15, Time: 509.90s, LR: 0.001000, Training Loss: 0.0087, Validation Loss: 0.1552\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 408/408 [00:21<00:00, 18.80it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Quick Test Loss: 0.1418\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1653/1653 [08:10<00:00,  3.37it/s]\n",
            "Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 368/368 [00:21<00:00, 17.48it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 16, Time: 511.62s, LR: 0.001000, Training Loss: 0.0074, Validation Loss: 0.1473\n",
            "New best model saved to /content/drive/MyDrive/Colab Notebooks/save_files/best_model8.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 408/408 [00:21<00:00, 18.78it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Quick Test Loss: 0.1376\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1653/1653 [08:08<00:00,  3.39it/s]\n",
            "Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 368/368 [00:20<00:00, 17.54it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 17, Time: 509.18s, LR: 0.001000, Training Loss: 0.0059, Validation Loss: 0.1454\n",
            "New best model saved to /content/drive/MyDrive/Colab Notebooks/save_files/best_model8.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 408/408 [00:21<00:00, 18.88it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Quick Test Loss: 0.1330\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1653/1653 [08:09<00:00,  3.38it/s]\n",
            "Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 368/368 [00:20<00:00, 17.60it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 18, Time: 510.15s, LR: 0.001000, Training Loss: 0.0052, Validation Loss: 0.1473\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 408/408 [00:21<00:00, 18.85it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Quick Test Loss: 0.1340\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1653/1653 [08:08<00:00,  3.38it/s]\n",
            "Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 368/368 [00:21<00:00, 17.45it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 19, Time: 509.62s, LR: 0.001000, Training Loss: 0.0044, Validation Loss: 0.1409\n",
            "New best model saved to /content/drive/MyDrive/Colab Notebooks/save_files/best_model8.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 408/408 [00:21<00:00, 18.87it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Quick Test Loss: 0.1308\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1653/1653 [08:08<00:00,  3.39it/s]\n",
            "Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 368/368 [00:20<00:00, 17.54it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 20, Time: 509.17s, LR: 0.001000, Training Loss: 0.0036, Validation Loss: 0.1443\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 408/408 [00:21<00:00, 18.81it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Quick Test Loss: 0.1321\n",
            "Final model saved to /content/drive/MyDrive/Colab Notebooks/save_files/model_final8.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 408/408 [00:21<00:00, 18.99it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Final Test Loss: 0.1315\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "import torch\n",
        "import os\n",
        "\n",
        "# Training and evaluation loop with early stopping\n",
        "best_val_loss = float('inf')\n",
        "patience = 5  # Number of epochs to wait after last time validation loss improved.\n",
        "counter = 0  # Counter for early stopping\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Training phase\n",
        "    train_loss = train(model, train_loader, criterion, optimizer, device, PAD_IDX)\n",
        "\n",
        "    # Validation phase\n",
        "    val_loss = validate(model, val_loader, criterion, device, PAD_IDX)\n",
        "\n",
        "    # Update learning rate\n",
        "    scheduler.step(val_loss)\n",
        "\n",
        "    # Log epoch details\n",
        "    epoch_time = time.time() - start_time\n",
        "    current_lr = optimizer.param_groups[0]['lr']\n",
        "    print(f\"Epoch: {epoch+1}, Time: {epoch_time:.2f}s, LR: {current_lr:.6f}, \"\n",
        "          f\"Training Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}\")\n",
        "\n",
        "    # Check for improvement in validation loss\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        best_model_path = os.path.join(SAVE_PATH, 'best_model.pth')\n",
        "        torch.save(model.state_dict(), best_model_path)\n",
        "        print(f\"Improved validation loss: {val_loss:.4f}. Saving model to {best_model_path}\")\n",
        "        counter = 0  # Reset counter after improvement\n",
        "    else:\n",
        "        counter += 1  # Increment counter if no improvement\n",
        "        print(f\"No improvement in validation loss for {counter} epoch(s).\")\n",
        "\n",
        "    # Early stopping check\n",
        "    if counter >= patience:\n",
        "        print(\"Stopping training early due to no improvement in validation loss.\")\n",
        "        break\n",
        "\n",
        "    # Quick evaluation on test data\n",
        "    quick_test_loss = evaluate(model, test_loader, criterion, device, PAD_IDX)\n",
        "    print(f\"Quick Test Loss: {quick_test_loss:.4f}\")\n",
        "\n",
        "# Load the best model for final evaluation\n",
        "model.load_state_dict(torch.load(best_model_path))\n",
        "print(f\"Loaded best model from {best_model_path} for final evaluation.\")\n",
        "\n",
        "# Final evaluation on the test data\n",
        "final_test_loss = evaluate(model, test_loader, criterion, device, PAD_IDX)\n",
        "print(f\"Final Test Loss: {final_test_loss:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generate function - to run when generation needed\n",
        "This script defines a function that generates a response verse to a given input verse using a trained model. The function tokenizes the input, feeds it through the model iteratively, and builds a response verse based on the model's predictions, using temperature scaling and top-k sampling for more nuanced and controlled text generation."
      ],
      "metadata": {
        "id": "bxalZndnlAo8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hgAz_8RS3JJO"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def generate_response(model, input_verse, tokenizer, max_length=50, temperature=1.0, topk=10):\n",
        "    \"\"\"\n",
        "    Generates a response verse given an input verse using the trained model.\n",
        "\n",
        "    Parameters:\n",
        "    model (nn.Module): The trained neural network model.\n",
        "    input_verse (str): The input verse to which the model will generate a response.\n",
        "    tokenizer: The tokenizer used for encoding and decoding the words.\n",
        "    max_length (int): The maximum length of the response verse.\n",
        "    temperature (float): The temperature for sampling. A higher value generates more random outputs.\n",
        "    topk (int): The number of top probable next words to consider for sampling.\n",
        "\n",
        "    Returns:\n",
        "    str: The generated response verse.\n",
        "    \"\"\"\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    hidden = model.init_hidden(1)  # Initialize the hidden state\n",
        "    device = next(model.parameters()).device  # Get the device of the model\n",
        "\n",
        "    # Tokenize the input verse. Unknown words are replaced with <unk> token\n",
        "    input_tokens = [tokenizer.word_index.get(w, tokenizer.word_index['<unk>']) for w in input_verse.split()]\n",
        "\n",
        "    # Generate response\n",
        "    response = []\n",
        "    for i in range(max_length):\n",
        "        with torch.no_grad():\n",
        "            # Convert input tokens to tensor and move to the model's device\n",
        "            input_tensor = torch.tensor([input_tokens], dtype=torch.long).to(device)\n",
        "            lengths = torch.tensor([len(input_tokens)], dtype=torch.long).to(device)\n",
        "\n",
        "            # Forward pass through the model\n",
        "            output, hidden = model(input_tensor, hidden, lengths)\n",
        "\n",
        "            # Apply temperature scaling to the last time-step output\n",
        "            last_output = output[0, -1, :].squeeze().div(temperature)\n",
        "            word_weights = F.softmax(last_output, dim=-1)  # Softmax to get probabilities\n",
        "\n",
        "            # Select topk indices to sample from\n",
        "            topk_indices = torch.topk(word_weights, topk).indices\n",
        "            chosen_word_index = topk_indices[torch.randint(0, topk, (1,))].item()\n",
        "\n",
        "            if chosen_word_index == tokenizer.word_index.get('<eos>', -1):\n",
        "                break  # Stop if end-of-sequence token is generated\n",
        "\n",
        "            # Append the chosen word to the response and update the input tokens\n",
        "            response.append(tokenizer.index_word.get(chosen_word_index, \"<unk>\"))\n",
        "            input_tokens.append(chosen_word_index)\n",
        "\n",
        "    return ' '.join(response)  # Join the response words into a single string\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generation execution\n",
        "This script focuses on loading a pre-trained model, evaluating it on test data, and then using it to generate responses to a given challenger verse. The responses vary based on different combinations of temperature and topk values, demonstrating the model's ability to create diverse and contextually relevant verses. The script is useful for testing and showcasing the capabilities of the trained model in a rap battle context."
      ],
      "metadata": {
        "id": "F7K_xftilXzI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GQBrjYF7TcxP",
        "outputId": "c515cae4-d643-4b95-9ff2-6ef92fa7efef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model loaded successfully.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 408/408 [00:08<00:00, 46.15it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Loss: 1.24\n",
            "\n",
            "Defender's response with temperature 0.7 and topk 10:\n",
            "him already fuck rap or would you any been emma again already should hell has keep ya over hey am next down something again heart you do i been but time be ass for been would ass made do i had from a some you been emma would him then\n",
            "\n",
            "Defender's response with temperature 0.7 and topk 12:\n",
            "back feel find there being that why over should well made do i being lines but going hell yeah should hell then flow make dead im but time a heart going heart been can or would back want words had you been do were fuckin with keep sick already my\n",
            "\n",
            "Defender's response with temperature 0.7 and topk 15:\n",
            "find think ya because going rap been do stop the has thats but yeah has thats am ass own little lyrics much something with some got you had can <defender> can <defender> i mind that been made because shit mean here dick from been then high should time high ass\n",
            "\n",
            "Defender's response with temperature 0.75 and topk 10:\n",
            "find because should ass yeah get want words being find ya his some top the flow the fuck hell rap been would next am next die but been emma over keep show being for you any heart been then my well am put fuckin little been over next made because\n",
            "\n",
            "Defender's response with temperature 0.75 and topk 12:\n",
            "find for leave fucking find down you i want been going make the niggas hey because why should i put you been going going stop thats yeah high i top much dead there words being that thats shit same then might there had she then him for back top own\n",
            "\n",
            "Defender's response with temperature 0.75 and topk 15:\n",
            "him some got she time top your should my time heart because back do time damn made down ass am something niggas again emma hard heart for find emma my should hell has put stop do been damn made down its fuck fuck no made why his be there was\n",
            "\n",
            "Defender's response with temperature 0.8 and topk 10:\n",
            "find make dick much im shit lyrics heart do you been over hey down thats shit top your put same do time been flow for him some a put being find do same mind that you stop or my heart do were kill mind has ass sick been emma would\n",
            "\n",
            "Defender's response with temperature 0.8 and topk 12:\n",
            "why thats yeah keep kill want rap you do same words got rapping be rapping do you give put ass much the hate emma again him for you fuckin do been but a going been but a was i want love got you same mind over should were show again\n",
            "\n",
            "Defender's response with temperature 0.8 and topk 15:\n",
            "him for why high any some ass that keep dead the there was stop or high some put ass flow you had damn lines but should put ass am i want rap or hard your back kill top dead dick that been flow because am back being down you had\n",
            "\n",
            "Defender's response with temperature 0.85 and topk 10:\n",
            "should my was you had you do you had but top much from get same dick over hey made why flow his my well you top your put can being from a was fuckin start make dead down its words been would thats but time words want or over hate\n",
            "\n",
            "Defender's response with temperature 0.85 and topk 12:\n",
            "why high put dead them keep fuckin bad show would love you any him really bitches now am something put had flow make next made start had or flow own made back hard dead ass own fuck hell your with fuck dick lines might i had or keep kill niggas\n",
            "\n",
            "Defender's response with temperature 0.85 and topk 15:\n",
            "find down dick much dead no has thats am you top there stop damn im am or has him already put i put you kill niggas flow rap got find make can were kill ya flow rap then raps im shit rap a put i want why next the his\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "\n",
        "# Load the best model\n",
        "best_model_path = os.path.join(SAVE_PATH, 'best_model7.pth')\n",
        "if os.path.exists(best_model_path):\n",
        "    # Load the state dictionary of the best model if it exists\n",
        "    model.load_state_dict(torch.load(best_model_path))\n",
        "    print(\"Model loaded successfully.\")\n",
        "else:\n",
        "    # Print a message if the model file does not exist\n",
        "    print(f\"Model not found in {best_model_path}. Ensure the path is correct.\")\n",
        "\n",
        "# Evaluate the model on test data\n",
        "test_loss = evaluate(model, test_loader, criterion, device, PAD_IDX)\n",
        "print(f'Test Loss: {test_loss:.2f}')  # Print the calculated test loss\n",
        "\n",
        "# Example Challenger verse for generating a response\n",
        "challenger_verse = \"i am the best in the world and you bitch who are you do you think you can win against me your dick is so small that nobody can see it\"\n",
        "\n",
        "# Define expanded ranges for temperature and topk values for response generation\n",
        "temperatures = [0.7, 0.75, 0.8, 0.85]  # Temperature values to control randomness\n",
        "topks = [10, 12, 15]  # Top-k values to limit choices in sampling\n",
        "\n",
        "# Generate and display responses for each combination of temperature and topk\n",
        "for temp in temperatures:\n",
        "    for topk in topks:\n",
        "        # Generate a response from the defender's perspective\n",
        "        defender_verse = generate_response(model, challenger_verse, tokenizer, max_length=50, temperature=temp, topk=topk)\n",
        "        print(f\"\\nDefender's response with temperature {temp} and topk {topk}:\")\n",
        "        print(defender_verse)  # Print the generated response\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "collapsed_sections": [
        "HDKvPfG20oxB",
        "PZXHZ4mw0VFa",
        "pGzrKXNQ0yQn",
        "Q3BpdAgY2nVD",
        "RO_7ClPH5U8h",
        "F6vDrlmSXAoi",
        "I8jLLKx_Yn8o",
        "DZ81v8RaaiVJ",
        "438H4jGcb3Kn",
        "CWGflaGKc9rX",
        "H81rNeOhfeiw",
        "yMJ9ZLVehy8m",
        "mZOtXDA3i2lh",
        "-JjEF74XjY7b",
        "bxalZndnlAo8",
        "F7K_xftilXzI"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}