{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VNuWm01ZjgX4"
      },
      "source": [
        "##Install & import the necessary librairy : to be launch each time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xceK2Zegii0W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9404a3ea-1bf4-424a-90cf-a771743204ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fuzzywuzzy\n",
            "  Downloading fuzzywuzzy-0.18.0-py2.py3-none-any.whl (18 kB)\n",
            "Installing collected packages: fuzzywuzzy\n",
            "Successfully installed fuzzywuzzy-0.18.0\n",
            "Collecting pyspellchecker\n",
            "  Downloading pyspellchecker-0.7.2-py3-none-any.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyspellchecker\n",
            "Successfully installed pyspellchecker-0.7.2\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "E: Unable to locate package enchant\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Collecting pyenchant\n",
            "  Downloading pyenchant-3.2.2-py3-none-any.whl (55 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.7/55.7 kB\u001b[0m \u001b[31m965.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n",
            "Installing collected packages: pyenchant\n",
            "Successfully installed pyenchant-3.2.2\n",
            "Collecting enchant\n",
            "  Downloading enchant-0.0.1-py3-none-any.whl (2.4 kB)\n",
            "Installing collected packages: enchant\n",
            "Successfully installed enchant-0.0.1\n",
            "Collecting autocorrect\n",
            "  Downloading autocorrect-2.6.1.tar.gz (622 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m622.8/622.8 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: autocorrect\n",
            "  Building wheel for autocorrect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for autocorrect: filename=autocorrect-2.6.1-py3-none-any.whl size=622364 sha256=981463d46e32ce7e689d896edf7ff481b7ae3d4afe21f3668ea795a8946ef7b1\n",
            "  Stored in directory: /root/.cache/pip/wheels/b5/7b/6d/b76b29ce11ff8e2521c8c7dd0e5bfee4fb1789d76193124343\n",
            "Successfully built autocorrect\n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0mCollecting langdetect\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from langdetect) (1.16.0)\n",
            "Building wheels for collected packages: langdetect\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993225 sha256=095cadb6d357cf6242a3d9890e8d3c11db60eac8eb37adaea39999430fa368fa\n",
            "  Stored in directory: /root/.cache/pip/wheels/95/03/7d/59ea870c70ce4e5a370638b5462a7711ab78fba2f655d05106\n",
            "Successfully built langdetect\n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0mRequirement already satisfied: pyspellchecker in /usr/local/lib/python3.10/dist-packages (0.7.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install fuzzywuzzy\n",
        "!pip install pyspellchecker\n",
        "!apt-get install -y enchant\n",
        "!pip install nltk pyenchant\n",
        "!pip install enchant\n",
        "!pip install autocorrect\n",
        "!pip install langdetect\n",
        "!pip install pyspellchecker\n",
        "!pip install spacy\n",
        "!python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hPrJIqS1j07y"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "except:\n",
        "    IN_COLAB = False\n",
        "\n",
        "if IN_COLAB:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "    # Import the required libraries\n",
        "from bs4 import BeautifulSoup  # For parsing HTML\n",
        "import requests  # For making HTTP requests\n",
        "import pandas as pd  # For data manipulation and analysis\n",
        "import time\n",
        "import requests\n",
        "import re\n",
        "import sqlite3\n",
        "from fuzzywuzzy import fuzz\n",
        "from autocorrect import Speller\n",
        "from langdetect import detect\n",
        "import nltk\n",
        "import langdetect\n",
        "\n",
        "import os, time, math\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import pprint  # for pretty printing\n",
        "pp = pprint.PrettyPrinter()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q1CVdDwPkXSa"
      },
      "source": [
        "##Data pre-cleaning process : rapper name variation : no need to launch again"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kww2tUKAkZ6M"
      },
      "outputs": [],
      "source": [
        "# code to see the rapper name variation in lyrics\n",
        "\n",
        "# Initialize a list to store the original names, their variations found in the lyrics, and the similarity levels\n",
        "name_variations_with_similarity = []\n",
        "\n",
        "# Function to find similar names within the lyrics\n",
        "def find_similar_names_in_lyrics(df):\n",
        "    for index, row in df.iterrows():\n",
        "        url = row['url']\n",
        "        challenger_name = row['challenger_name']\n",
        "        defender_name = row['defender_name']\n",
        "        challenger_lyric = row['challenger_lyrics']\n",
        "        defender_lyric = row['defender_lyrics']\n",
        "\n",
        "        # Check for similar names within the challenger lyrics\n",
        "        challenger_words = challenger_lyric.split()\n",
        "        for word in challenger_words:\n",
        "            similarity_level = fuzz.ratio(challenger_name.lower(), word.lower())\n",
        "            if 80 <= similarity_level <= 99:\n",
        "                name_variations_with_similarity.append((url, 'Challenger', challenger_name, word, similarity_level))\n",
        "\n",
        "        # Check for similar names within the defender lyrics\n",
        "        defender_words = defender_lyric.split()\n",
        "        for word in defender_words:\n",
        "            similarity_level = fuzz.ratio(defender_name.lower(), word.lower())\n",
        "            if 80 <= similarity_level <= 99:\n",
        "                name_variations_with_similarity.append((url, 'Defender', defender_name, word, similarity_level))\n",
        "\n",
        "    # Convert the list to a DataFrame\n",
        "    df_similar_names = pd.DataFrame(name_variations_with_similarity, columns=['URL', 'Role', 'Original_Name', 'Variation', 'Similarity_Level'])\n",
        "\n",
        "    return df_similar_names\n",
        "\n",
        "# Sample code to run the function (assuming 'sample_df_for_names' is your DataFrame)\n",
        "df_similar_names = find_similar_names_in_lyrics(df_battles)\n",
        "\n",
        "# Sample code to save the DataFrame to a Tableau file\n",
        "df_similar_names.to_csv('/content/drive/MyDrive/Colab Notebooks/tableau_file.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KIgqMyF4knen"
      },
      "source": [
        "## Cleaning functions : to launch each time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_F6qloIckrUt"
      },
      "outputs": [],
      "source": [
        "# Step -1 : Convert data into dataframe\n",
        "# Function to convert the entire 'battles' table in the SQLite database to a Pandas DataFrame\n",
        "def import_and_convert_to_dataframe(db_path, table_name):\n",
        "    # Connect to the SQLite database\n",
        "    conn = sqlite3.connect(db_path)\n",
        "    # Fetch all data from the table into a Pandas DataFrame\n",
        "    query = f\"SELECT * FROM {table_name};\"\n",
        "    df = pd.read_sql(query, conn)\n",
        "    # Close the database connection\n",
        "    conn.close()\n",
        "    return df\n",
        "\n",
        "# Step 0 : replace rapper name variation with the original name\n",
        "def replace_rapper_name_variations_in_lyric(lyric, role, cleaned_df, url):\n",
        "    # Extract the relevant rows from cleaned_df based on the URL and role\n",
        "    relevant_rows = cleaned_df[(cleaned_df['URL'] == url) & (cleaned_df['Role'].str.lower() == role.lower())]\n",
        "\n",
        "    # Replace variations with the original names in the lyric\n",
        "    for _, row in relevant_rows.iterrows():\n",
        "        original_name = row['Original_Name']\n",
        "        variation = row['Variation']\n",
        "        lyric = lyric.replace(variation, original_name)\n",
        "\n",
        "    return lyric\n",
        "\n",
        "import re\n",
        "\n",
        "def step_replace_self_references(lyric, self_name):\n",
        "    # Compile the regex pattern for an exact match.\n",
        "    # The \\b ensures we match full words only. re.IGNORECASE makes the match case-insensitive.\n",
        "    pattern = re.compile(rf'\\b{re.escape(self_name)}\\b', re.IGNORECASE)\n",
        "\n",
        "    # Replace the self_name with 'xxxxmyselfxxxx' wherever it appears as a full word in the lyric.\n",
        "    modified_lyric = pattern.sub('xxxxmyselfxxxx', lyric)\n",
        "\n",
        "    return modified_lyric\n",
        "\n",
        "def step_replace_opponent_references(lyric, opponent_name):\n",
        "    # Compile the regex pattern for an exact match.\n",
        "    # The \\b ensures we match full words only. re.IGNORECASE makes the match case-insensitive.\n",
        "    pattern = re.compile(rf'\\b{re.escape(opponent_name)}\\b', re.IGNORECASE)\n",
        "\n",
        "    # Replace the opponent_name with 'xxxxoponentxxxx' wherever it appears as a full word in the lyric.\n",
        "    modified_lyric = pattern.sub('xxxxoponentxxxx', lyric)\n",
        "\n",
        "    return modified_lyric\n",
        "\n",
        "\n",
        "# Step : Convert to lowercase\n",
        "def step_lowercase(lyric):\n",
        "    return lyric.lower()\n",
        "\n",
        "# Step : remove any trailing special characters, including \"_\", \".\", and \"-\", at the end of lines, while keeping \"!\", \"?\", and \")\".\n",
        "def step_remove_trailing_special_chars(lyric):\n",
        "    # Remove \"#\" symbols, even if they are attached to words\n",
        "    lyric = re.sub(r'#', '', lyric)\n",
        "\n",
        "    # Remove special characters like \"_\", \".\", \"-\", and \",\" from the end of each line\n",
        "    lyric = re.sub(r'[_\\.,-]$', '', lyric, flags=re.MULTILINE)\n",
        "\n",
        "    # Remove any other unwanted special characters at the end of lines, while keeping \"!\", \"?\", and \")\"\n",
        "    lyric = re.sub(r'[^!?)\\w\\s]$', '', lyric, flags=re.MULTILINE)\n",
        "\n",
        "    return lyric\n",
        "\n",
        "# Step 5: Replace ellipsis (\"...\", \"..\", \"....\", etc.) with a space\n",
        "def step_replace_ellipsis(lyric):\n",
        "    return re.sub(r'\\.{2,}', ' ', lyric)\n",
        "\n",
        "# Step 6: Remove special characters including emojis\n",
        "def step_remove_special_chars(lyric):\n",
        "    return re.sub(r'[@#$%^*+=|\\\\<>\\/]', '', lyric)\n",
        "\n",
        "# Step 7: Remove URLs\n",
        "def step_remove_urls(lyric):\n",
        "    return re.sub(r'http\\S+|www\\S+|https\\S+', '', lyric, flags=re.MULTILINE)\n",
        "\n",
        "# Step 8: Remove lines like \"//Verse1\" and \"Verse X\"\n",
        "def step_remove_verse_labels(lyric):\n",
        "    lyric = re.sub(r'//.*', '', lyric)\n",
        "    return re.sub(r'\\nverse \\d+', '', lyric, flags=re.IGNORECASE)\n",
        "\n",
        "# Step 9: Remove lines with only one word\n",
        "def step_remove_one_word_lines(lyric):\n",
        "    lines = lyric.strip().split('\\n')\n",
        "    cleaned_lines = [line for line in lines if len(line.split()) > 1]\n",
        "    return '\\n'.join(cleaned_lines)\n",
        "\n",
        "# Step 10: Remove lyrics with fewer than 4 lines (considered as spam)\n",
        "def step_remove_short_lyrics(lyric):\n",
        "    lines = lyric.strip().split('\\n')\n",
        "    if len(lines) < 4:\n",
        "        return None  # Returning None\n",
        "    return '\\n'.join(lines)\n",
        "\n",
        "# Step 11: Remove consecutive duplicate lines, keeping just one\n",
        "def step_remove_consecutive_duplicate_lines(lyric):\n",
        "    lines = lyric.strip().split('\\n')\n",
        "    unique_lines = []\n",
        "    prev_line = None\n",
        "\n",
        "    for line in lines:\n",
        "        if line != prev_line:\n",
        "            unique_lines.append(line)\n",
        "        prev_line = line\n",
        "\n",
        "    return '\\n'.join(unique_lines)\n",
        "\n",
        "# Step 12: Remove consecutive spaces while preserving line breaks\n",
        "def step_remove_consecutive_spaces(lyric):\n",
        "    if lyric is not None:\n",
        "        lines = lyric.split('\\n')\n",
        "        cleaned_lines = []\n",
        "\n",
        "        for line in lines:\n",
        "            cleaned_line = ' '.join(line.split())\n",
        "            cleaned_lines.append(cleaned_line)\n",
        "\n",
        "        return '\\n'.join(cleaned_lines)\n",
        "    else:\n",
        "        return None  # Return None if the lyric is None\n",
        "\n",
        "# Step 13: Tokenization\n",
        "def step_tokenize(lyric):\n",
        "    lines = lyric.strip().split('\\n')\n",
        "    tokenized_lines = [' '.join(word_tokenize(line)) for line in lines]\n",
        "    return '\\n'.join(tokenized_lines)\n",
        "\n",
        "# Step 14: Spelling Correction\n",
        "def step_spell_correction(lyric):\n",
        "    special_words = {'xxxxoponentxxxx', 'xxxxmyselfxxxx'}\n",
        "\n",
        "    lines = lyric.strip().split('\\n')\n",
        "    corrected_lines = []\n",
        "\n",
        "    for line in lines:\n",
        "        words = line.split()\n",
        "        corrected_words = []\n",
        "\n",
        "        for word in words:\n",
        "            if word in special_words:\n",
        "                corrected_words.append(word)\n",
        "            else:\n",
        "                corrected_word = spell.correction(word)\n",
        "                if corrected_word is not None:\n",
        "                    corrected_words.append(corrected_word)\n",
        "                else:\n",
        "                    corrected_words.append(word)\n",
        "\n",
        "        corrected_line = ' '.join(corrected_words)\n",
        "        corrected_lines.append(corrected_line)\n",
        "\n",
        "    return '\\n'.join(corrected_lines)\n",
        "\n",
        "\n",
        "\n",
        "# Step : Lemmatization with spaCy\n",
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def step_lemmatize(lyric):\n",
        "    special_words = {'xxxxoponentxxxx', 'xxxxmyselfxxxx'}\n",
        "\n",
        "    lines = lyric.strip().split('\\n')\n",
        "    lemmatized_lines = []\n",
        "\n",
        "    for line in lines:\n",
        "        words = line.split()\n",
        "        #print(f\"Debug: Before Lemmatization: {words}\")  # Debug statement\n",
        "\n",
        "        # Process each line with spaCy\n",
        "        doc = nlp(line)\n",
        "\n",
        "        # Lemmatize each token\n",
        "        lemmatized_words = [token.lemma_ if token.text not in special_words else token.text for token in doc]\n",
        "\n",
        "        #print(f\"Debug: After Lemmatization: {lemmatized_words}\")  # Debug statement\n",
        "\n",
        "        lemmatized_line = ' '.join(lemmatized_words)\n",
        "        lemmatized_lines.append(lemmatized_line)\n",
        "\n",
        "    return '\\n'.join(lemmatized_lines)\n",
        "\n",
        "#Step : abbreviation\n",
        "def expand_abbreviations(lyric):\n",
        "    abbreviations = {\n",
        "        \" u \": \" you \",\n",
        "        \" ur \": \" your \",\n",
        "        \" cuz \": \" because \",\n",
        "        \" cos \": \" because \",\n",
        "        \" yr \": \" your \",\n",
        "        \" y'all \": \" you all \",\n",
        "        \" ain't \": \" am not \",\n",
        "        \" gonna \": \" going to \",\n",
        "        \" wanna \": \" want to \",\n",
        "        \" gotta \": \" got to \",\n",
        "        \" til \": \" until \",\n",
        "        \" b4 \": \" before \",\n",
        "        \" bout \": \" about \",\n",
        "        \" luv \": \" love \",\n",
        "        \" thx \": \" thanks \",\n",
        "        \" pls \": \" please \",\n",
        "        \" plz \": \" please \",\n",
        "        \" fam \": \" family \",\n",
        "        \" idk \": \" I do not know \",\n",
        "        \" imo \": \" in my opinion \",\n",
        "        \" imho \": \" in my humble opinion \",\n",
        "        \" brb \": \" be right back \",\n",
        "        \" gtg \": \" got to go \",\n",
        "        \" btw \": \" by the way \",\n",
        "        \" omg \": \" oh my god \",\n",
        "        \" smh \": \" shaking my head \",\n",
        "        \" tbh \": \" to be honest \",\n",
        "        \" idc \": \" I do not care \",\n",
        "        \" wyd \": \" what you doing \",\n",
        "        \" hmu \": \" hit me up \",\n",
        "        \" dm \": \" direct message \",\n",
        "        \" yo \": \" hey \",\n",
        "        \" bro \": \" brother \",\n",
        "        \" sis \": \" sister \",\n",
        "        \" r \": \" are \",\n",
        "        \" n \": \" and \",\n",
        "        \"'m \": \" am \",\n",
        "        \"'ll \": \" will \",\n",
        "        \" ill \" : \" I will \",\n",
        "        \"'ve \": \" have \",\n",
        "        \"'re \": \" are \",\n",
        "        \"n't \": \" not \",\n",
        "        \"'s \": \" is \",  # Note: this is ambiguous; could mean \"has\" or \"is\"\n",
        "        \" i' \": \" I \",\n",
        "        \" i'am \": \" I am \",\n",
        "        \" i \" : \" I \",\n",
        "        \" im \": \" I am \",\n",
        "        \" im' \" : \" I am \",\n",
        "        \" wi \": \" we \",\n",
        "        \" don't \": \" do not \",\n",
        "        \" dont \": \" do not \",\n",
        "        \" doesn't \": \" does not \",\n",
        "        \" doesnt \": \" does not \",\n",
        "        \" won't\":\" will not \",\n",
        "        \" make'em \": \" make them\",\n",
        "        \" ta \": \" to \",\n",
        "        \" em \": \" them \",\n",
        "        \" ca not \" : \" cannot \",\n",
        "        \" wont\" : \" will not \"\n",
        "\n",
        "    }\n",
        "\n",
        "    for abbr, full in abbreviations.items():\n",
        "        lyric = lyric.lower().replace(abbr, full)\n",
        "\n",
        "    return lyric\n",
        "\n",
        "\n",
        "# Step : remove non standart word\n",
        "def step_remove_non_standard_words(lyric, standard_dict, custom_dict):\n",
        "    special_words = {'xxxxoponentxxxx', 'xxxxmyselfxxxx'}\n",
        "    lines = lyric.strip().split('\\n')\n",
        "    cleaned_lines = []\n",
        "    removed_words = []  # List to keep track of removed words\n",
        "\n",
        "    for line in lines:\n",
        "        words = line.split()\n",
        "        cleaned_words = []\n",
        "\n",
        "        for word in words:\n",
        "            if word in standard_dict or word in custom_dict or word in special_words:\n",
        "                cleaned_words.append(word)\n",
        "            else:\n",
        "                removed_words.append(word)\n",
        "\n",
        "        cleaned_line = ' '.join(cleaned_words)\n",
        "        cleaned_lines.append(cleaned_line)\n",
        "\n",
        "    return '\\n'.join(cleaned_lines), removed_words\n",
        "\n",
        "# Step 18: Remove Stop Words\n",
        "def step_remove_stop_words(lyric):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    lines = lyric.strip().split('\\n')\n",
        "    filtered_lines = []\n",
        "    for line in lines:\n",
        "        words = line.split()\n",
        "        filtered_words = [word for word in words if word.lower() not in stop_words]\n",
        "        filtered_line = ' '.join(filtered_words)\n",
        "        filtered_lines.append(filtered_line)\n",
        "    return '\\n'.join(filtered_lines)\n",
        "\n",
        "# Step 19: Handle Contractions\n",
        "def step_handle_contractions(lyric):\n",
        "    contractions_dict = {\"I'll\": \"I will\", \"they're\": \"they are\"}  # Add more\n",
        "    lines = lyric.strip().split('\\n')\n",
        "    expanded_lines = []\n",
        "    for line in lines:\n",
        "        for contraction, expansion in contractions_dict.items():\n",
        "            line = line.replace(contraction, expansion)\n",
        "        expanded_lines.append(line)\n",
        "    return '\\n'.join(expanded_lines)\n",
        "\n",
        "# Step 20: Remove Numbers\n",
        "def step_remove_numbers(lyric):\n",
        "    return re.sub(r'\\b\\d+\\b', '', lyric)\n",
        "\n",
        "# Step 21: Remove Short Words\n",
        "def step_remove_short_words(lyric):\n",
        "    lines = lyric.strip().split('\\n')\n",
        "    filtered_lines = []\n",
        "    for line in lines:\n",
        "        words = line.split()\n",
        "        filtered_words = [word for word in words if len(word) > 2]\n",
        "        filtered_line = ' '.join(filtered_words)\n",
        "        filtered_lines.append(filtered_line)\n",
        "    return '\\n'.join(filtered_lines)\n",
        "\n",
        "# Step 22 : Remove Punctuation\n",
        "def step_remove_punctuation(lyric):\n",
        "    return re.sub(r'[^\\w\\s]', '', lyric)\n",
        "\n",
        "# Step 23 : Language Detection\n",
        "def step_language_detection(lyric):\n",
        "    try:\n",
        "        if langdetect.detect(lyric) == 'en':\n",
        "            return lyric\n",
        "    except langdetect.lang_detect_exception.LangDetectException:\n",
        "        pass\n",
        "    return None  # Return None if the lyric is not English or if there was an error\n",
        "\n",
        "\n",
        "# Step 24: Convert British English to American English\n",
        "def step_british_to_american(lyric, british_to_american_path):\n",
        "    df = pd.read_csv(british_to_american_path)\n",
        "    british_to_american_dict = dict(zip(df['British'], df['American']))\n",
        "    for british, american in british_to_american_dict.items():\n",
        "        pattern = re.compile(rf'\\b{british}\\b', re.IGNORECASE)\n",
        "        lyric = pattern.sub(american, lyric)\n",
        "    return lyric\n",
        "\n",
        "# Final cleaning function using individual steps (Optimized Order)\n",
        "def clean_lyric_optimized(lyric, self_name, opponent_name, url, cleaned_df, standard_dict, custom_dict, british_to_american_path):\n",
        "    # Check if the lyric has fewer than 3 words\n",
        "    if len(lyric.split()) < 3:\n",
        "        return None, []\n",
        "\n",
        "    # Step 1: Language Detection\n",
        "    lyric = step_language_detection(lyric)\n",
        "    if lyric is None:\n",
        "        return None, []\n",
        "\n",
        "    # Step 2: Replace Rapper Name Variations\n",
        "    lyric = replace_rapper_name_variations_in_lyric(lyric, 'Challenger', cleaned_df, url)\n",
        "    # Step 3: Replace Self and Opponent References\n",
        "    lyric = step_replace_self_references(lyric, self_name)\n",
        "    lyric = step_replace_opponent_references(lyric, opponent_name)\n",
        "    # Step 4: Lowercase Conversion\n",
        "    lyric = step_lowercase(lyric)\n",
        "    # Step 5: Handle Contractions\n",
        "    lyric = step_handle_contractions(lyric)\n",
        "    # Step 6: Remove URLs, Special Characters, Emojis, Trailing Chars, Ellipsis, Verse Labels, One-Word Lines\n",
        "    lyric = step_remove_urls(lyric)\n",
        "    lyric = step_remove_special_chars(lyric)\n",
        "    lyric = step_remove_trailing_special_chars(lyric)\n",
        "    lyric = step_replace_ellipsis(lyric)\n",
        "    lyric = step_remove_verse_labels(lyric)\n",
        "    lyric = step_remove_one_word_lines(lyric)\n",
        "    # Step 7: Remove Consecutive Duplicate Lines and Spaces\n",
        "    lyric = step_remove_consecutive_duplicate_lines(lyric)\n",
        "    lyric = step_remove_consecutive_spaces(lyric)\n",
        "    lyric = step_remove_numbers(lyric)\n",
        "    # Step 8: Spelling Correction\n",
        "    lyric = step_spell_correction(lyric)\n",
        "    lyric = expand_abbreviations(lyric)\n",
        "    # Step 9: Lemmatization\n",
        "    #lyric = step_lemmatize(lyric)\n",
        "\n",
        "    # Step 10: Tokenization\n",
        "    #lyric = step_tokenize(lyric)\n",
        "\n",
        "    # Step 11: Remove Non-Standard Words, Stop Words, Numbers, Short Words, Punctuation\n",
        "    lyric, removed_words = step_remove_non_standard_words(lyric, standard_dict, custom_dict)\n",
        "\n",
        "    #lyric = step_remove_stop_words(lyric)\n",
        "    #lyric = step_remove_short_words(lyric)\n",
        "    lyric = step_remove_punctuation(lyric)\n",
        "\n",
        "    # Step 12: Convert British to American English\n",
        "    lyric = step_british_to_american(lyric, british_to_american_path)\n",
        "\n",
        "    # Step 13: Remove Short Lyrics\n",
        "    #lyric = step_remove_short_lyrics(lyric)\n",
        "\n",
        "    return lyric, removed_words  # return both the cleaned lyric and the list of removed words\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h7-c8dqdkwS7"
      },
      "source": [
        "Import dictionary : to launch each time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5XCbMxu-kyEz"
      },
      "outputs": [],
      "source": [
        "# Import the 'battles' table and convert it to a DataFrame\n",
        "db_path = '/content/drive/MyDrive/Colab Notebooks/data.db'\n",
        "table_name = 'battles'\n",
        "df_battles = import_and_convert_to_dataframe(db_path, table_name)\n",
        "\n",
        "# Import bristish to american mapping\n",
        "british_to_american_path = '/content/drive/MyDrive/Colab Notebooks/British_to_American_Mappings.csv'\n",
        "\n",
        "# Import the cleaned DataFrame rapper name variation (cleaned manually)\n",
        "cleaned_csv_path = \"/content/drive/MyDrive/Colab Notebooks/Rapper name variation cleaned.csv\"\n",
        "df_cleaned_filtered = pd.read_csv(cleaned_csv_path)\n",
        "\n",
        "# Import Urban Rap dictionnay\n",
        "custom_dict_path = '/content/drive/MyDrive/Colab Notebooks/urbandict-word-defs.csv'\n",
        "custom_dict = pd.read_csv(custom_dict_path, error_bad_lines=False)['word'].str.lower().tolist()\n",
        "custom_dict = set(custom_dict)  # Converting list to set for faster lookup\n",
        "\n",
        "# nltk_words is the nltk English dictionary\n",
        "nltk.download('words')\n",
        "from nltk.corpus import words\n",
        "standard_dict = set(words.words())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AOGsWEK6k-MS"
      },
      "source": [
        "## Create word removed and cleaning lyric : to launch when needed time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m8sftcy2lDxa"
      },
      "outputs": [],
      "source": [
        "# Import necessary modules\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "from spellchecker import SpellChecker\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Initialize SpellChecker\n",
        "spell = SpellChecker()\n",
        "\n",
        "# Sample 100 rows from the DataFrame\n",
        "sample_df = df_battles.sample(n=100)\n",
        "\n",
        "# Initialize an empty list for storing removed words\n",
        "removed_words = []\n",
        "\n",
        "# Open text files for storing cleaned lyrics and removed words\n",
        "with open(\"/content/drive/MyDrive/Colab Notebooks/original_and_cleaned_lyrics.txt\", \"w\") as f, \\\n",
        "     open(\"/content/drive/MyDrive/Colab Notebooks/removed_words.txt\", \"w\") as removed_f, \\\n",
        "     open(\"/content/drive/MyDrive/Colab Notebooks/corpus_cleaned_lyrics.txt\", \"w\") as corpus_f:\n",
        "\n",
        "    for index, row in sample_df.iterrows():\n",
        "        try:\n",
        "            # Extract data from the DataFrame row\n",
        "            url = row['url']\n",
        "            challenger_name = row['challenger_name']\n",
        "            defender_name = row['defender_name']\n",
        "            original_challenger_lyric = row['challenger_lyrics']\n",
        "            original_defender_lyric = row['defender_lyrics']\n",
        "\n",
        "            # Clean the lyrics for the challenger\n",
        "            cleaned_challenger_lyric, removed_words_challenger = clean_lyric_optimized(\n",
        "                original_challenger_lyric, challenger_name, defender_name, url, df_cleaned_filtered, standard_dict, custom_dict, british_to_american_path)\n",
        "\n",
        "            # Clean the lyrics for the defender\n",
        "            cleaned_defender_lyric, removed_words_defender = clean_lyric_optimized(\n",
        "                original_defender_lyric, defender_name, challenger_name, url, df_cleaned_filtered, standard_dict, custom_dict, british_to_american_path)\n",
        "\n",
        "            # Write cleaned lyrics to the file if they are not None\n",
        "            if cleaned_challenger_lyric and cleaned_defender_lyric:\n",
        "                f.write(f\"Challenger: {challenger_name}\\n\")\n",
        "                for orig_line, cleaned_line in zip(original_challenger_lyric.split('\\n'), cleaned_challenger_lyric.split('\\n')):\n",
        "                    f.write(f\"Original: {orig_line}\\n\")\n",
        "                    f.write(f\"Cleaned: {cleaned_line}\\n\")\n",
        "\n",
        "                f.write(f\"Defender: {defender_name}\\n\")\n",
        "                for orig_line, cleaned_line in zip(original_defender_lyric.split('\\n'), cleaned_defender_lyric.split('\\n')):\n",
        "                    f.write(f\"Original: {orig_line}\\n\")\n",
        "                    f.write(f\"Cleaned: {cleaned_line}\\n\")\n",
        "\n",
        "                f.write(\"=\" * 50 + \"\\n\")\n",
        "\n",
        "            # Write cleaned lyrics to the corpus file\n",
        "            if cleaned_challenger_lyric and cleaned_defender_lyric:\n",
        "                corpus_f.write(cleaned_challenger_lyric + \"\\n\")\n",
        "                corpus_f.write(cleaned_defender_lyric + \"\\n\")\n",
        "\n",
        "            # Write removed words to a separate file\n",
        "            removed_f.write(f\"Challenger: {challenger_name}\\n\")\n",
        "            removed_f.write(\"\\n\".join(removed_words_challenger) + \"\\n\")\n",
        "            removed_f.write(f\"Defender: {defender_name}\\n\")\n",
        "            removed_f.write(\"\\n\".join(removed_words_defender) + \"\\n\")\n",
        "            removed_f.write(\"=\" * 50 + \"\\n\")\n",
        "\n",
        "        except Exception as e:\n",
        "            # Log exceptions\n",
        "            removed_f.write(f\"Error at index {index}: {e}\\n\")\n",
        "            removed_f.write(\"=\" * 50 + \"\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "auxV2aPIwI7E"
      },
      "source": [
        "## create corpus only"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xYg-cKJvwKen"
      },
      "outputs": [],
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "import math\n",
        "from spellchecker import SpellChecker\n",
        "import pandas as pd\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Initialize SpellChecker\n",
        "spell = SpellChecker()\n",
        "\n",
        "# Initialize new columns for cleaned lyrics\n",
        "df_battles['cleaned_challenger_lyric'] = ''\n",
        "df_battles['cleaned_defender_lyric'] = ''\n",
        "\n",
        "# Number of samples in each batch\n",
        "batch_size = 5000\n",
        "\n",
        "# Calculate total batches\n",
        "total_batches = math.ceil(len(df_battles) / batch_size)\n",
        "\n",
        "# Loop through all batches\n",
        "for batch in range(3, total_batches):  # Adjust range as needed\n",
        "\n",
        "    # Calculate the start and end indices for the current batch\n",
        "    start_idx = batch * batch_size\n",
        "    end_idx = min((batch + 1) * batch_size, len(df_battles))\n",
        "\n",
        "    # Create a new file for each batch\n",
        "    file_name = f\"/content/drive/MyDrive/Colab Notebooks/corpus_cleaned_lyrics_batch_{batch}.txt\"\n",
        "    with open(file_name, \"w\") as corpus_f:\n",
        "\n",
        "        # Loop through the rows in the current batch\n",
        "        for idx, (index, row) in enumerate(df_battles.iloc[start_idx:end_idx].iterrows()):\n",
        "\n",
        "            try:\n",
        "                # Extract data from the DataFrame row\n",
        "                url = row['url']\n",
        "                challenger_name = row['challenger_name']\n",
        "                defender_name = row['defender_name']\n",
        "                original_challenger_lyric = row['challenger_lyrics']\n",
        "                original_defender_lyric = row['defender_lyrics']\n",
        "\n",
        "                # Clean the lyrics for the challenger\n",
        "                cleaned_challenger_lyric, _ = clean_lyric_optimized(\n",
        "                    original_challenger_lyric, challenger_name, defender_name, url, df_cleaned_filtered, standard_dict, custom_dict, british_to_american_path)\n",
        "                cleaned_challenger_lyric = cleaned_challenger_lyric or \" \"\n",
        "\n",
        "                # Clean the lyrics for the defender\n",
        "                cleaned_defender_lyric, _ = clean_lyric_optimized(\n",
        "                    original_defender_lyric, defender_name, challenger_name, url, df_cleaned_filtered, standard_dict, custom_dict, british_to_american_path)\n",
        "                cleaned_defender_lyric = cleaned_defender_lyric or \" \"\n",
        "\n",
        "                # Update the DataFrame\n",
        "                df_battles.at[index, 'cleaned_challenger_lyric'] = cleaned_challenger_lyric\n",
        "                df_battles.at[index, 'cleaned_defender_lyric'] = cleaned_defender_lyric\n",
        "\n",
        "                # Write cleaned lyrics to the corpus file\n",
        "                corpus_f.write(cleaned_challenger_lyric + \"\\n\")\n",
        "                corpus_f.write(cleaned_defender_lyric + \"\\n\")\n",
        "\n",
        "                # Print progress\n",
        "                if (idx + 1) % 5000 == 0:\n",
        "                    print(f\"Processed {start_idx + idx + 1} rows.\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error at index {start_idx + idx}: {e}\")\n",
        "\n",
        "    # Save the updated DataFrame to a new CSV file, unique for each batch\n",
        "    df_battles.iloc[start_idx:end_idx].to_csv(f\"/content/drive/MyDrive/Colab Notebooks/battles_cleaned_batch_{batch}.csv\", index=False)\n",
        "\n",
        "    # Optional: Print completion of batch\n",
        "    print(f\"Batch {batch} completed and saved.\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "q1CVdDwPkXSa",
        "AOGsWEK6k-MS"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}