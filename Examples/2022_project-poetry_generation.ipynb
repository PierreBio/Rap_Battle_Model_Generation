{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23KpIezdSpnn"
      },
      "source": [
        "<center><img src=\"data:image/png;base64,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\" /></center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vI4q50sCSpn3"
      },
      "source": [
        "<h1><center><strong><font color=\"chillipepper\">IA 717: Poetry Generation Project</font></strong></center></h1>\n",
        "<h3><center><font color=\"blue\"><strong>Student Version</strong></font></center></h3>\n",
        "\n",
        "<center>\n",
        "<h3> Project Supervisor <br/> Cyril Chhun</h3>\n",
        "<email>cyril.chhun@telecom-paris.fr</email>\n",
        "<br/>\n",
        "Year 2022-2023\n",
        "</center>\n",
        "\n",
        "------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vq1amhcU4Vga",
        "jp-MarkdownHeadingCollapsed": true,
        "tags": []
      },
      "source": [
        "\n",
        "# ⚠ IMPORTANT ⚠"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3rVhPG1iL90U"
      },
      "source": [
        "Start by creating your own copy of this notebook: in the top menu, select `File` and then `Save a copy in Drive` (or the equivalent instructions in French). Make sure that it created a copy in a folder named `Colab Notebooks` at the root of your Google Drive.\n",
        "\n",
        "Also, when you train the model (so, not for now; a reminder will be issued when it is relevant), you should select a GPU runtime for faster training. In the `Runtime` menu, you can do so by selecting `Change runtime type` and choose `GPU`. Google Colab restricts GPU usage so wait until you actually need it.\n",
        "\n",
        "GitHub File"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3tlj2ffC9LKF"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "except:\n",
        "    IN_COLAB = False\n",
        "\n",
        "if IN_COLAB:\n",
        "    from google.colab import drive\n",
        "\n",
        "\n",
        "    drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gDaLnGFIbG8a"
      },
      "source": [
        "Now, inside the `Colab Notebooks` folder, create two empty folders named `dramacode_corpus` and `dramacode_checkpoints`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ks4SAM0p9Yf7"
      },
      "outputs": [],
      "source": [
        "if IN_COLAB:\n",
        "    ROOT_PATH = '/content/drive/MyDrive/Colab Notebooks/'\n",
        "else:\n",
        "    ROOT_PATH = './'\n",
        "DRAMACODE_PATH = ROOT_PATH + \"dramacode.github.io/\"\n",
        "NAKED_PATH = DRAMACODE_PATH + \"naked/\"\n",
        "CORPUS_PATH = ROOT_PATH + \"dramacode_corpus\"\n",
        "CHECKPOINT_PATH = ROOT_PATH + \"dramacode_checkpoints\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_3wHdtYJL7Cu",
        "tags": []
      },
      "source": [
        "# Poetry generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eaV8-6rIMIcf",
        "jp-MarkdownHeadingCollapsed": true,
        "tags": []
      },
      "source": [
        "## Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0zZjsJtKMJko"
      },
      "source": [
        "In this project, we will tackle automatic French poetry generation.\n",
        "\n",
        "Since this task is pretty ambitious, we will need rather complex language models. We will be using recurrent neural networks (RNN), and particularly Long-Short Term Memory (LSTM) cells."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F3NNDxk4LLlI"
      },
      "source": [
        "### Task Definition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2VdqpXyfY8xG"
      },
      "source": [
        "Let us begin with defining our task more precisely.\n",
        "\n",
        "In this project, we will focus on verse, specifically [alexandrines](https://www.wikiwand.com/fr/Alexandrin), i.e. twelve-syllable verse.\n",
        "\n",
        "For example, this famous line from Racine's *Phèdre* is one of them:\n",
        "> Tout m'afflige et me nuit, et conspire à me nuire.\n",
        "\n",
        "We will also want these verses to rhyme in pairs, in what we call \"flat\" rhymes, i.e., of the form AABB(CC...).\n",
        "\n",
        "> Tout m'afflige et me nuit, et conspire à me nuire.  \n",
        "> Comme on voit tous ses vœux l’un l’autre se détruire !  \n",
        "> Vous-même, condamnant vos injustes desseins,  \n",
        "> Tantôt à vous parer vous excitiez nos mains ;\n",
        "\n",
        "If you have time and motivation left, you will find at the end of this notebook some additional work tracks.\n",
        "\n",
        "Of course, you are also free to look for others on your own, should you feel like it!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVp8wO-DLPA6"
      },
      "source": [
        "### On neural networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q-IlziLSLf8n"
      },
      "source": [
        "#### Basic facts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZSiqvXBKLjA2"
      },
      "source": [
        "An in-depth study of neural networks is not the object of this project, but it can be useful to have a rough idea of how they work. We will present them here briefly; the curious reader can consult the sources cited below, refer to the Deep Learning course of the curriculum or search by themslves, on the Internet for example.\n",
        "\n",
        "The perceptron, also called artificial neuron or formal neuron, tries to reproduce the function of a biological neuron. The objective of a neural network is to reproduce an arbitrarily complex function that associates an input $x$ with an output $y$. For example, we can create neural networks to recognize a dog from a cat: the input is then an image, and the output the word \"dog\" or \"cat\".\n",
        "\n",
        "A layer of a neural network is governed by the following equation:\n",
        "\n",
        "$$ \\hat{y} = f(\\mathbf{W} \\mathbf{X} + b) $$\n",
        "\n",
        "where\n",
        "- $\\mathbf{X}$ is the input matrix, usually consisting of several vectors $x_1, \\dots, x_m$\n",
        "- $\\hat{y}$ is the output vector\n",
        "- $\\mathbf{W}$ is a matrix of weights (parameters specific to the layer, which can be updated)\n",
        "- $b$ is a vector of weights called *bias* (also subject to evolution)\n",
        "- $f$ is a function called *activation function*, generally non-linear like the sigmoid, $\\mathrm{ReLU}$ or $\\tanh$ functions.\n",
        "\n",
        "![](https://user.oc-static.com/upload/2018/12/10/15444553183515_neuroneformel-1.png)\n",
        "\n",
        "The main thing to remember is that the layers of a neural network combine inputs $x_1, \\dots, x_m$ by linear (the $\\mathbf{W} \\mathbf{X} + b$) and non-linear ($f$) operations in the hope of obtaining an output $y$.\n",
        "\n",
        "In practice, at the initialization of the network, given an input $x$, we usually obtain an output $\\hat{y}$ which can be very different from $y$ : the parameters $\\mathbf{W}$ and $b$ are not yet appropriate.\n",
        "\n",
        "We then perform a *training* of the neural network which allows to reduce the distance between $\\hat{y}$ and $y$, i.e. to minimize the *loss function* $\\mathcal{L}(y, \\hat{y}) = \\| \\hat{y} - y \\|^2$. This is usually done by an optimization process such as gradient descent. We will skip the technical details (look at the sources for more information); just remember that by updating the parameters $\\mathbf{W}$ and $b$ incrementally, it is possible to improve the performance of the network as it will produce a $\\hat{y}$ similar to the desired $y$.\n",
        "\n",
        "![](https://1.cms.s81c.com/sites/default/files/2021-01-06/ICLH_Diagram_Batch_01_04-GradientDescent-WHITEBG.png)\n",
        "\n",
        "By stacking such layers, the network becomes more complex, hence, more elaborate mechanisms can be approximated. However, this usually requires a longer and more data-intensive training. The \"*Deep*\" in *Deep Learning* reflects the fact that we use multi-layered neural networks.\n",
        "\n",
        "As an illustration, here is a neural network that learns to separate crosses and circles: the more the training progresses (a step is commonly called *epoch*), the better it works.\n",
        "\n",
        "![](https://user.oc-static.com/upload/2018/12/12/15446484526497_linearsep_anim.gif)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xfQEqxtmLnuC"
      },
      "source": [
        "##### Sources"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hGY1hba_Lq5V"
      },
      "source": [
        "- [OpenClassrooms - Initiez-vous au Deep Learning](https://openclassrooms.com/fr/courses/5801891-initiez-vous-au-deep-learning)\n",
        "- [IBM - What are neural networks?](https://www.ibm.com/cloud/learn/neural-networks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Npnvxn8Ls5i"
      },
      "source": [
        "#### Recurrent neural networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QYPnMYkKbR41"
      },
      "source": [
        "Recurrent neural networks (RNN) are neural networks that are particularly adapted to process sequential data such as time series or text. Indeed, in a text, there are sequential dependencies between words; we cannot write a sentence in any order, in French or English at least.\n",
        "\n",
        "The difference between RNNs and classical neural networks (usually called *feed-forward neural networks*) is that the former have a \"memory\" represented by a hidden state that evolves along a \"time\" axis in any given layer.\n",
        "\n",
        "![](https://miro.medium.com/max/875/1*AQ52bwW55GsJt6HTxPDuMA.gif)\n",
        "![](https://miro.medium.com/max/875/1*o-Cq5U8-tfa1_ve2Pf3nfg.gif)\n",
        "![](https://miro.medium.com/max/875/1*WMnFSJHzOloFlJHU6fVN-g.gif)\n",
        "\n",
        "There are many ways to implement such a memory; here we will use a type of RNN called LSTM (Long-Short Term Memory), which was the best performing model until the introduction of the *transformer* in 2017 (which *transformer* will not be on the agenda of this project; the curious reader may read the corresponding paper in the sources).\n",
        "\n",
        "![](https://miro.medium.com/max/875/1*0f8r3Vd-i4ueYND1CUrhMA.png)\n",
        "\n",
        "The LSTM exhibits several interesting mechanisms; in particular, it has the ability to choose what it wishes to retain or forget in the long term. Refer to the sources for more detail.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xm_4Cpy7L0Gf"
      },
      "source": [
        "##### Sources"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NsqeOTlwLwSY"
      },
      "source": [
        "- [colah's blog - Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n",
        "- [Towards data science - Illustrated Guide to LSTM’s and GRU’s: A step by step explanation](https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21)\n",
        "- [Distill - Attention and Augmented Recurrent Neural Networks](https://distill.pub/2016/augmented-rnns/)\n",
        "- [Transformer - Attention Is All You Need](https://arxiv.org/abs/1706.03762)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fRQm3DwlMMz-",
        "jp-MarkdownHeadingCollapsed": true,
        "tags": []
      },
      "source": [
        "## Data retrieval"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jedRZgXOerit"
      },
      "source": [
        "To begin with, we will have to retrieve a set of data relevant to our task.\n",
        "\n",
        "Since we want to generate alexandrines, we will have to retrieve texts in alexandrines. Luckily, a certain amount of French plays are available on  the [Dramacode](https://dramacode.github.io/) website.\n",
        "\n",
        "Copy the following code in the empty cell below and run it to mount your Google Drive on this notebook and download the dataset. Once it is done, delete the cell so that you do not run it again by mistake.\n",
        "\n",
        "```python\n",
        "!cd /content/drive/MyDrive/\"Colab Notebooks\" && wget -r -A '*.txt' dramacode.github.io/\n",
        "```\n",
        "\n",
        "You should find a `dramacode.github.io` folder in your `Colab Notebooks` folder. We will be using the `txt` files from the `naked` folder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vYQvW8qcMmVd"
      },
      "outputs": [],
      "source": [
        "# copy the code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NQmRK2CIRNYJ",
        "jp-MarkdownHeadingCollapsed": true,
        "tags": []
      },
      "source": [
        "## Data preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bBhoUv1WbnBX",
        "tags": []
      },
      "source": [
        "### Explanations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7u-sQUPG1ggp"
      },
      "source": [
        "Now that we have retrieved our data, we will need to preprocess it. There are a few operations we might want to do:\n",
        "- Filter the files to keep only those which are in alexandrines\n",
        "- Remove the punctuation or add spaces around\n",
        "- Put every character to lower case\n",
        "- Stem the words\n",
        "- Lemmatize the words\n",
        "- Remove stop words\n",
        "\n",
        "Below we will create the functions for each of them. However, you are free to chose which ones you will use; for some, it is difficult to predict if they will improve the results. For others, there are good reasons to choose them, or not. Ideally, you could test every possible combination, but that would take a lot of time.\n",
        "\n",
        "Just keep in mind that we want to generate **natural** French language."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5bDjh1HcRLiN",
        "tags": []
      },
      "source": [
        "### Text filtering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M93yoM0UJwia"
      },
      "source": [
        "We will only do some basic filtering based for instance on the length of the line.\n",
        "You could open `corneillep_cid.txt` and check the average length of the lines for example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T3ZIr1biLwoM"
      },
      "outputs": [],
      "source": [
        "# histogrma de nombre de lignes en function alexandra (en terme de mots ou characteres)\n",
        "# retirer pontuation espace,\n",
        "# mayuscules\n",
        "#"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eg3RvHDFJwA1"
      },
      "outputs": [],
      "source": [
        "def keep_alexandrines_only(lines):\n",
        "    \"\"\"\n",
        "        Given an array of lines, returns the filtered array\n",
        "        containing lines which are the most likely to be alexandrines.\n",
        "    \"\"\"\n",
        "    # TODO\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ol0PAL0NJk5"
      },
      "outputs": [],
      "source": [
        "def test_keep_alexandrines_only():\n",
        "    \"\"\"ideally, the first section should have much fewer lines than the second part\"\"\"\n",
        "    print(\"--- NOT ALEXANDRINES ---\")\n",
        "    with open(os.path.join(NAKED_PATH, \"allainval_ecoledesbourgeois.txt\")) as fin:\n",
        "        test_lines = keep_alexandrines_only(fin.readlines()[:50])\n",
        "        for line in test_lines:\n",
        "            print(line)\n",
        "    print(\"\\n--- ALEXANDRINES ---\")\n",
        "    with open(os.path.join(NAKED_PATH, \"corneillep_cid.txt\")) as fin:\n",
        "        test_lines = keep_alexandrines_only(fin.readlines()[:50])\n",
        "        for line in test_lines:\n",
        "            print(line)\n",
        "\n",
        "test_keep_alexandrines_only()  # feel free to comment once you're satisfied"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pDWKQ95KoR8n"
      },
      "source": [
        "#### Expected result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JdwizD5uoLOl"
      },
      "source": [
        "```\n",
        "--- NOT ALEXANDRINES ---\n",
        "Est-ce que tu songes encore à Damis ?\n",
        "Madame, voilà Monsieur Mathieu qui vient d’entrer.\n",
        "Vous craignez qu’il ne goûte pas cette alliance ?\n",
        "Oh ! Mademoiselle n’en tient point.\n",
        "Il ne donnera jamais son consentement.\n",
        "\n",
        "--- ALEXANDRINES ---\n",
        "Entre tous ces amants dont la jeune ferveur\n",
        "Adore votre fille, et brigue ma faveur,\n",
        "Don Rodrigue et Don Sanche à l’envi font paraître\n",
        "Le beau feu qu’en leurs coeurs ses beautés ont fait naître,\n",
        "Ce n’est pas que Chimène écoute leurs soupirs\n",
        "Ou d’un regard propice anime leurs désirs,\n",
        "Au contraire pour tout dedans l’indifférence\n",
        "Elle n’ôte à pas un, ni donne d’espérance,\n",
        "Et sans les voir d’un oeil trop sévère, ou trop doux,\n",
        "C’est de votre seul choix qu’elle attend un époux.\n",
        "Elle est dans le devoir, tous deux sont dignes d’elle,\n",
        "Tous deux formés d’un sang, noble, vaillants, fidèle,\n",
        "Jeunes, mais qui font lire aisément dans leurs yeux\n",
        "L’éclatante vertu de leurs braves aïeux.\n",
        "Don Rodrigue sur tout n’a trait en son visage\n",
        "Qui d’un homme de Cour ne soit la haute image,\n",
        "Et sort d’une maison si seconde en guerriers,\n",
        "Qu’ils y prennent naissance au milieu des lauriers,\n",
        "La valeur de son père, en son temps sans pareille,\n",
        "Tant qu’a duré sa force a passé pour merveille,\n",
        "Ses rides sur son front ont gravé ses exploits\n",
        "Et nous disent encor ce qu’il fut autrefois :\n",
        "Je me promets du fils ce que j’ai vu du père,\n",
        "Et ma fille en un mot peut l’aimer et me plaire.\n",
        "Va l’en entretenir, mais dans cet entretien\n",
        "Cache mon sentiment et découvre le sien,\n",
        "Je veux qu’à mon retour nous en parlions ensemble ;\n",
        "L’heure à présent m’appelle au conseil qui s’assemble,\n",
        "Le Roi doit à son fils choisir un gouverneur,\n",
        "Ou plutôt m’élever à ce haut rang d’honneur,\n",
        "Ce que pour lui mon bras chaque jour exécute,\n",
        "Me défend de penser qu’aucun me le dispute.\n",
        "Quelle douce nouvelle à ces jeunes amants !\n",
        "Et que tout se dispose à leurs contentements !\n",
        "Et bien, Elvire, enfin, que faut-il que j’espère ?\n",
        "Que dois-je devenir, et que t’as dit mon père ?\n",
        "Deux mots dont tous vos sens doivent être charmés,\n",
        "Il estime Rodrigue autant que vous l’aimez.\n",
        "L’excès de ce bonheur me met en défiance,\n",
        "Puisse à de tels discours donner quelque croyance ?\n",
        "Il passe bien plus outre, il approuve ses feux,\n",
        "Et vous doit commander de répondre à ses voeux.\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GvPRl_5BSF8g"
      },
      "source": [
        "As you can see, a few lines from the non-verse file sneaked through our rudimentary filter. To improve this, you could for instance compute the percentage of lines kept with respect to the original number of lines: if the ratio is too small, you could choose to keep nothing at all since it is likely that the file is not even in verse."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oneyl4MpRQCn",
        "tags": []
      },
      "source": [
        "### Punctuation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idg6UkZRhqvs",
        "tags": []
      },
      "source": [
        "#### Explanations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZq1yRxiRSG9"
      },
      "source": [
        "Here we will create functions to either remove punctuation or put spaces around it for better handling later. Let us write both, and you will choose which you want to do. (There is no good or bad answer)\n",
        "\n",
        "[Relevant documentation](https://docs.python.org/3/library/string.html#string.punctuation)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PsR1S4kvTutH",
        "tags": []
      },
      "source": [
        "#### Punctuation spacing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eSz2d815RmXi"
      },
      "outputs": [],
      "source": [
        "def space_punctuation(lines):\n",
        "    \"\"\"\n",
        "        given an array of lines, returns the lines with spaced punctuation.\n",
        "        Caution: inverted commas should ideally not be preceded by spaces.\n",
        "        For instance, \"j'ai faim\" should become \"j' ai faim\".\n",
        "        It is possible that the `’` symbol is used instead of `'` in the data.\n",
        "        The str.replace() and str.split() function might help.\n",
        "    \"\"\"\n",
        "    # TODO\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5tZnK6SHT8CT"
      },
      "outputs": [],
      "source": [
        "def test_space_punctuation():\n",
        "    with open(os.path.join(NAKED_PATH, \"corneillep_cid.txt\")) as fin:\n",
        "        test_lines = space_punctuation(fin.readlines()[2:10])\n",
        "        for line in test_lines:\n",
        "            print(line)\n",
        "\n",
        "test_space_punctuation()  # feel free to comment once you're satisfied"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BuqGrC9xhn9H"
      },
      "source": [
        "##### Expected result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o9dYFBYCoENh"
      },
      "source": [
        "```\n",
        "Entre tous ces amants dont la jeune ferveur\n",
        "Adore votre fille , et brigue ma faveur ,\n",
        "Don Rodrigue et Don Sanche à l’ envi font paraître\n",
        "Le beau feu qu’ en leurs coeurs ses beautés ont fait naître ,\n",
        "Ce n’ est pas que Chimène écoute leurs soupirs\n",
        "Ou d’ un regard propice anime leurs désirs ,\n",
        "Au contraire pour tout dedans l’ indifférence\n",
        "Elle n’ ôte à pas un , ni donne d’ espérance ,\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ndOLkfWTxtr"
      },
      "source": [
        "#### Punctuation removal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LW8Zpq6KV5dE"
      },
      "outputs": [],
      "source": [
        "def remove_punctuation(lines):\n",
        "    \"\"\"\n",
        "        given an array of lines, returns the lines without punctuation.\n",
        "        You might want to keep inverted commas and a space after them, though.\n",
        "        The str.replace() and str.split() function might help.\n",
        "    \"\"\"\n",
        "    # TODO\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "56Ywcd2oWKQ-"
      },
      "outputs": [],
      "source": [
        "def test_remove_punctuation():\n",
        "    with open(os.path.join(NAKED_PATH, \"corneillep_cid.txt\")) as fin:\n",
        "        test_lines = remove_punctuation(fin.readlines()[2:10])\n",
        "        for line in test_lines:\n",
        "            print(line)\n",
        "\n",
        "test_remove_punctuation()  # feel free to comment once you're satisfied"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4KgPg4lLhkqz"
      },
      "source": [
        "##### Expected result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_kkWSCpn-0t"
      },
      "source": [
        "```\n",
        "Entre tous ces amants dont la jeune ferveur\n",
        "Adore votre fille et brigue ma faveur\n",
        "Don Rodrigue et Don Sanche à l’ envi font paraître\n",
        "Le beau feu qu’ en leurs coeurs ses beautés ont fait naître\n",
        "Ce n’ est pas que Chimène écoute leurs soupirs\n",
        "Ou d’ un regard propice anime leurs désirs\n",
        "Au contraire pour tout dedans l’ indifférence\n",
        "Elle n’ ôte à pas un ni donne d’ espérance\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SMul6gQcXHTo",
        "tags": []
      },
      "source": [
        "### Lower case"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oVCd0AtMXKxg"
      },
      "source": [
        "This does not really need testing, since [a function already exists](https://docs.python.org/3/library/stdtypes.html?highlight=lower#str.lower)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I8cnuePYSa1U",
        "tags": []
      },
      "source": [
        "### Stemming"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U4CJd84VSdEY"
      },
      "source": [
        "You might want to use the NLTK package. [Relevant documentation](https://www.nltk.org/api/nltk.stem.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FtiMMu2aTRIk"
      },
      "outputs": [],
      "source": [
        "def stem_lines(lines):\n",
        "    # TODO\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zYveNxzWT1Wd"
      },
      "outputs": [],
      "source": [
        "def test_stem_lines():\n",
        "    with open(os.path.join(NAKED_PATH, \"corneillep_cid.txt\")) as fin:\n",
        "        test_lines = stem_lines(fin.readlines()[2:10])\n",
        "        for line in test_lines:\n",
        "            print(line)\n",
        "\n",
        "test_stem_lines()  # feel free to comment once you're satisfied"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ni03DlR9hf7_"
      },
      "source": [
        "#### Expected result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JCqxLmR-n2rH"
      },
      "source": [
        "```\n",
        "entre tous ce amant dont la jeun ferveur\n",
        "ador votr fille, et brigu ma faveur,\n",
        "don rodrigu et don sanch à l’env font paraîtr\n",
        "le beau feu qu’en leur coeur se beaut ont fait naître,\n",
        "ce n’est pas que chimen écout leur soupir\n",
        "ou d’un regard propic anim leur désirs,\n",
        "au contrair pour tout dedan l’indifférent\n",
        "elle n’ôt à pas un, ni don d’espérance,\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oOJl9RBJiHa6",
        "tags": []
      },
      "source": [
        "### Lemmatization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q-Ih-rZHiI2C"
      },
      "source": [
        "You might want to use [SpaCy's lemmatizer](https://spacy.io/api/lemmatizer) here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ze3Snxp7jS6g"
      },
      "outputs": [],
      "source": [
        "def lemmatize_lines(lines):\n",
        "    # TODO\n",
        "    pass\n",
        "    return(new_lines)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VmhLV5Rlk6bN"
      },
      "outputs": [],
      "source": [
        "def test_lemmatize_lines():\n",
        "    with open(os.path.join(NAKED_PATH, \"corneillep_cid.txt\")) as fin:\n",
        "        test_lines = lemmatize_lines(fin.readlines()[2:10])\n",
        "        for line in test_lines:\n",
        "            print(line)\n",
        "\n",
        "test_lemmatize_lines()  # feel free to comment once you're satisfied"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ABc1YTWhTT1"
      },
      "source": [
        "#### Expected result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fwJXlZJ_ntuy"
      },
      "source": [
        "```\n",
        "entre tout ce amant dont le jeune ferveur\n",
        "Adore votre fille , et brigue mon faveur ,\n",
        "Don Rodrigue et Don sanche à l’ envi faire paraître\n",
        "le beau feu qu’ en leur coeur son beauté avoir faire naître ,\n",
        "ce n’ être pas que chimène écout leur soupir\n",
        "ou d’ un regard propice anime leur désir ,\n",
        "au contraire pour tout dedans l’ indifférence\n",
        "lui n’ ôte à pas un , ni donne d’ espérance ,\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9OtXBdZfXjPj",
        "tags": []
      },
      "source": [
        "### Combining the files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NWiFBYmxmzWL"
      },
      "source": [
        "We have several preprocessing functions at our disposal. Now, you can choose which ones you want to use to create the final corpus.\n",
        "\n",
        "Again, remember that we want to generate **natural** French language."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vc_uWAJZXnye"
      },
      "outputs": [],
      "source": [
        "def create_corpus():\n",
        "    # TODO\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rNxVW8OhZ_IH"
      },
      "outputs": [],
      "source": [
        "# create_corpus()  # uncomment and run once, then comment again"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DWXOObOZeRHc"
      },
      "source": [
        "You should obtain a file `corpus.txt` which is about 700,000 lines long (but it might be noticeably more or less depending on your filtering choices)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9IA19Ebm-93v",
        "tags": []
      },
      "source": [
        "### Splitting the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xwr84mMi_AKB"
      },
      "source": [
        "Now we will split the data in three datasets: `train`, `valid` and `test`. The `train` dataset will be used to train the model, ie, update its parameters. The `valid` dataset will be used to monitor the evolution of the loss during training, and the `test` dataset will be used to evaluate the model after training. We could do without the `test` dataset here since we will be looking at the generated output ourselves, but this is a common procedure so let us follow it.\n",
        "\n",
        "For instance, every 100 lines could be split as 80 / 10 / 10 or 90 / 5 / 5."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c4CeOWeq_TRI"
      },
      "outputs": [],
      "source": [
        "def split_corpus():\n",
        "    # TODO\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q-NKMi19B2Jb"
      },
      "outputs": [],
      "source": [
        "split_corpus()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCKFi6rS94Wk",
        "tags": []
      },
      "source": [
        "## Implementing the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "csNjTtrXfpVi",
        "jp-MarkdownHeadingCollapsed": true,
        "tags": []
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WRnfqi6Q99qB"
      },
      "source": [
        "We will be using the [PyTorch](https://pytorch.org/docs/stable/index.html) framework. For a detailed explanation about PyTorch, please refer to their [Learn the Basics tutorial](https://pytorch.org/tutorials/beginner/basics/intro.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1-7Ll_syEPZQ"
      },
      "outputs": [],
      "source": [
        "import os, time, math\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import pprint  # for pretty printing\n",
        "pp = pprint.PrettyPrinter()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n55a8GU_Nt-Q"
      },
      "outputs": [],
      "source": [
        "# use CUDA if on a GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)  # should be `cpu` for now, we will change runtime later"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "An6qm3I0GBkT",
        "jp-MarkdownHeadingCollapsed": true,
        "tags": []
      },
      "source": [
        "### Building the dictionary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QYbzSbLQGF3b"
      },
      "source": [
        "First, we need to build a dictionary so that we can pass words as unique identifiers, usually integers, to the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dC7vyFpqGE4v"
      },
      "outputs": [],
      "source": [
        "class Dictionary(object):\n",
        "    def __init__(self):\n",
        "        self.word2idx = {}  # if word2idx[\"hello\"] == 42 ...\n",
        "        self.idx2word = []  # ... then idx2word[42] == \"hello\"\n",
        "\n",
        "    def add_word(self, word):\n",
        "        \"\"\"\n",
        "        This function should check if the word is in word2idx; if it\n",
        "        is not, it should add it with the first available index\n",
        "        \"\"\"\n",
        "        # TODO\n",
        "        pass\n",
        "\n",
        "        return self.word2idx[word]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.idx2word)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "djfrHUstHk4C",
        "jp-MarkdownHeadingCollapsed": true,
        "tags": []
      },
      "source": [
        "### Building the corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uj_ACXMxHqPA",
        "tags": []
      },
      "outputs": [],
      "source": [
        "class Corpus(object):\n",
        "    def __init__(self, path):\n",
        "        # We create an object Dictionary associated to Corpus\n",
        "        self.dictionary = Dictionary()\n",
        "        # We go through all files, adding all words to the dictionary\n",
        "        self.train = self.tokenize(os.path.join(path, 'train.txt'))\n",
        "        self.valid = self.tokenize(os.path.join(path, 'valid.txt'))\n",
        "        self.test = self.tokenize(os.path.join(path, 'test.txt'))\n",
        "        self.eos_token = '<eos>'  # end of senetence (line) token\n",
        "\n",
        "    def tokenize(self, path):\n",
        "        \"\"\"\n",
        "            Tokenizes a text file, knowing the dictionary, in order to\n",
        "            tranform it into a list of indices.\n",
        "            The str.split() function might be useful.\n",
        "        \"\"\"\n",
        "        assert os.path.exists(path)\n",
        "\n",
        "        # Add words to the dictionary\n",
        "        # Do not forget to add the sos and eos tokens too\n",
        "\n",
        "        # TODO\n",
        "        pass\n",
        "\n",
        "        # Once done, effectively tokenize by adding the tokens to a vector.\n",
        "        # We want the `ids` vector to be an int64 torch tensor containing all\n",
        "        # tokens in the order of the file.\n",
        "        # Lines should all end with the sos token.\n",
        "\n",
        "        # TODO\n",
        "        pass\n",
        "\n",
        "        return ids"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DoECuQBCOlBh",
        "jp-MarkdownHeadingCollapsed": true,
        "tags": []
      },
      "source": [
        "### Loading the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KnQoQejCOj8t"
      },
      "outputs": [],
      "source": [
        "corpus = None  # TODO, create the corpus"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "knuYPoKlPgdP"
      },
      "source": [
        "### Batching the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YQvUUfMdf5Kf",
        "tags": []
      },
      "source": [
        "#### `batchify`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y56wY9aiPi4w"
      },
      "source": [
        "Our `corpus.train`, `corpus.valid` and `corpus.test` tensors are flat tensors; they cannot efficiently be processed with a gpu.\n",
        "We will therefore want to change their shape so that they are more square-like.\n",
        "\n",
        "With the alphabet being our data, we currently have the sequence:\n",
        "`[a b c d e f g h i j k l m n o p q r s t u v w x y z]`. We want to reorganize it as independant batches that will be processed independently by the model !\n",
        "\n",
        "For instance, with the alphabet as the sequence and batch size 4, we'd get the 4 following sequences (represented as columns):\n",
        "```\n",
        "┌ a g m s ┐\n",
        "│ b h n t │\n",
        "│ c i o u │\n",
        "│ d j p v │\n",
        "│ e k q w │\n",
        "└ f l r x ┘\n",
        "```\n",
        "with the last two elements being lost.\n",
        "\n",
        "Again, these columns are treated as independent by the model, which means that the dependence of e. g. 'g' on 'f' can not be learned, but allows more efficient processing.\n",
        "\n",
        "❓ **Question**: what do you think of this batching process? For instance, is it adapted to our data? If yes, how so? If not, how could we improve it?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ikqKozdCQUZi"
      },
      "outputs": [],
      "source": [
        "def batchify(data, bsz):\n",
        "    \"\"\"\n",
        "        Three steps:\n",
        "        1. Work out how cleanly we can divide the dataset into bsz parts.\n",
        "        2. Trim off any extra elements that wouldn't cleanly fit (remainders).\n",
        "        3. Evenly divide the data across the bsz batches.\n",
        "        Note: You might need to use `.contiguous()` at the end because PyTorch can be somewhat strict about memory usage.\n",
        "    \"\"\"\n",
        "\n",
        "    # TODO\n",
        "    pass\n",
        "\n",
        "    return data.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bfFr-ev9SKv9"
      },
      "outputs": [],
      "source": [
        "def test_batchify():\n",
        "    data = torch.tensor(np.arange(26))\n",
        "    print(batchify(data, 4))\n",
        "\n",
        "test_batchify()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HAfYXoJsk_iB"
      },
      "source": [
        "##### Expected result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "18bmaIOdfag4"
      },
      "source": [
        "\n",
        "```\n",
        "tensor([[ 0,  6, 12, 18],\n",
        "        [ 1,  7, 13, 19],\n",
        "        [ 2,  8, 14, 20],\n",
        "        [ 3,  9, 15, 21],\n",
        "        [ 4, 10, 16, 22],\n",
        "        [ 5, 11, 17, 23]])\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFqgElWQf-oN"
      },
      "source": [
        "#### `get_batch` function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ErZUp5ceHAs"
      },
      "source": [
        "Now we will build a function `get_batch` which subdivides the source data into chunks of the appropriate length. This function will be used later in the model training and evaluation.\n",
        "\n",
        "If source is equal to the example output of the batchify function, with\n",
        "a sequence length (seq_len) of 3, we'd get the following two variables:\n",
        "\n",
        "```\n",
        "┌ a g m s ┐ ┌ b h n t ┐\n",
        "| b h n t | | c i o u │\n",
        "└ c i o u ┘ └ d j p v ┘\n",
        "```\n",
        "\n",
        "The first variable contains the letters input to the network, while the second\n",
        "contains the one we want the network to predict (b for a, h for g, v for u, etc..)\n",
        "\n",
        "Note that despite the name of the function, the subdivison of data is not done along the batch dimension (i.e. dimension 1), since that was handled by the `batchify` function. The chunks are along dimension 0, corresponding to the `seq_len` dimension in the LSTM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xM_hcoPaeEpK"
      },
      "outputs": [],
      "source": [
        "def get_batch(source, i, seq_len):\n",
        "    \"\"\"\n",
        "        Should return (data, target) where data would be the first variable of\n",
        "        the example above, and target the second variable.\n",
        "\n",
        "        - source is the source data;\n",
        "        - i is the position of the current sequence;\n",
        "        - seq_len is the sequence length;\n",
        "\n",
        "        Three steps:\n",
        "        1. Deal with the possibility that there's not enough data left for a full sequence\n",
        "        2. Take the input data\n",
        "        3. Shift by one for the target data\n",
        "    \"\"\"\n",
        "\n",
        "    # TODO\n",
        "    pass\n",
        "\n",
        "    return data, target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sWzCPKSegdEF"
      },
      "outputs": [],
      "source": [
        "def test_get_batch():\n",
        "    source = torch.tensor(np.arange(26))\n",
        "    source = batchify(source, 4)\n",
        "    pp.pprint(get_batch(source, 2, 3))\n",
        "\n",
        "test_get_batch()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UVOaoTI9lTZ4"
      },
      "source": [
        "##### Expected result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zu6SpjVBlU7d"
      },
      "source": [
        "```\n",
        "(tensor([[ 2,  8, 14, 20],\n",
        "        [ 3,  9, 15, 21],\n",
        "        [ 4, 10, 16, 22]]),\n",
        " tensor([[ 3,  9, 15, 21],\n",
        "        [ 4, 10, 16, 22],\n",
        "        [ 5, 11, 17, 23]]))\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HEBXfAYWgHeQ"
      },
      "source": [
        "#### Batchifying the corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MZlA4vbATaNh"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 32  # you can choose other values\n",
        "EVAL_BATCH_SIZE = 16  # you can choose other values\n",
        "\n",
        "train_data = None\n",
        "val_data = None\n",
        "test_data = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qtAA00OrT9bA"
      },
      "outputs": [],
      "source": [
        "print(corpus.train.shape, train_data.shape)\n",
        "print(corpus.valid.shape, val_data.shape)\n",
        "print(corpus.test.shape, test_data.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xXnac_6vlqeQ"
      },
      "source": [
        "##### Expected result (your own corpus lengths and batch sizes may be different)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X1DAP2E5lyWK"
      },
      "source": [
        "```\n",
        "torch.Size([6987667]) torch.Size([54591, 128])\n",
        "torch.Size([874006]) torch.Size([54625, 16])\n",
        "torch.Size([872488]) torch.Size([54530, 16])\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "raxxbwZxWM3v"
      },
      "source": [
        "### Building the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EfSjvjtlWl4N"
      },
      "source": [
        "Models are usually implemented as custom nn.Module subclasses.\n",
        "- We need to redefine the __init__ method, which creates the object.\n",
        "- We also need to redefine the forward method, which transform the input into outputs.\n",
        "- We can also add any method that we need: here, in order to initiate weights in the model.\n",
        "\n",
        "Relevant documentation [here](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html?highlight=lstm#torch.nn.LSTM).\n",
        "\n",
        "Below is the model we will use; since you are not very familiar with how neural networks work yet, it is already filled and you can use it as a black box. However, you may want to look at it and try to understand how it works. Feel free to discuss about it together; if needed, you can also ask me questions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vro0grY6WKUB"
      },
      "outputs": [],
      "source": [
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, ntoken, ninp, nhid, nlayers, dropout=0.2, initrange=0.1):\n",
        "        \"\"\"\n",
        "            ntoken: length of the dictionary,\n",
        "            ninp: dimension of the input,\n",
        "            nhid: dimension of the hidden layers,\n",
        "            nlayers: number of layers,\n",
        "            dropout: regularization parameter\n",
        "            initrange: range for weight initialization\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.ntoken = ntoken\n",
        "        self.nhid = nhid\n",
        "        self.nlayers = nlayers\n",
        "        self.initrange = initrange\n",
        "        # Create a dropout object to use on layers for regularization\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        # Create an encoder - which is an embedding layer\n",
        "        self.encoder = nn.Embedding(ntoken, ninp)\n",
        "        # Create the LSTM layers - find out how to stack them !\n",
        "        self.rnn = nn.LSTM(ninp, nhid, nlayers, dropout=dropout)\n",
        "        # Create what we call the decoder: a linear transformation to map the hidden state into scores for all words in the vocabulary\n",
        "        # (Note that the softmax application function will be applied out of the model)\n",
        "        self.decoder = nn.Linear(nhid, ntoken)\n",
        "\n",
        "        # Initialize non-recurrent weights\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        # Initialize the encoder and decoder weights with the uniform distribution,\n",
        "        # between -self.initrange and +self.initrange, and the decoder bias with zeros\n",
        "        # https://pytorch.org/docs/stable/nn.init.html?highlight=init\n",
        "        # - the methods uniform_() and zeros_() might help\n",
        "        # - self.encoder has a .weight attribute\n",
        "        # - self.decoder has .weight and .bias attributes\n",
        "\n",
        "        nn.init.uniform_(self.encoder.weight, -self.initrange, self.initrange)\n",
        "        nn.init.zeros_(self.decoder.bias)\n",
        "        nn.init.uniform_(self.decoder.weight, -self.initrange, self.initrange)\n",
        "\n",
        "    def init_hidden(self, bsz):\n",
        "        weight = next(self.parameters())\n",
        "        return (weight.new_zeros(self.nlayers, bsz, self.nhid),\n",
        "                weight.new_zeros(self.nlayers, bsz, self.nhid))\n",
        "\n",
        "    def forward(self, input, hidden1):\n",
        "\n",
        "        # Process the input with the encoder, then dropout\n",
        "        emb = self.drop(self.encoder(input))\n",
        "\n",
        "        # Apply the LSTMs\n",
        "        output, hidden2 = self.rnn(emb, hidden1)\n",
        "\n",
        "        # Decode into scores\n",
        "        output = self.drop(output)\n",
        "        decoded = self.decoder(output)\n",
        "\n",
        "        return decoded, hidden2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NVJbvgKxU9jY"
      },
      "source": [
        "❓ **Question**: how does the method `LSTMModel.init_hidden` work?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9m83eDABcUE8",
        "tags": []
      },
      "source": [
        "## Running the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gabjdocxSppI"
      },
      "source": [
        "In this section, everything is already implemented. However, there are a few questions below that you should try to answer so as to understand what is going on."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9t-V77T9cZP3",
        "jp-MarkdownHeadingCollapsed": true,
        "tags": []
      },
      "source": [
        "### Initializing the parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qQCAADxqaIud"
      },
      "outputs": [],
      "source": [
        "SEED = 42  # you can choose other values\n",
        "torch.manual_seed(SEED) # set the random seed manually for reproducibility."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rYjCCAam0psk"
      },
      "outputs": [],
      "source": [
        "EMBEDDING_SIZE = 512  # you can choose other values\n",
        "HIDDEN_SIZE = 1024  # you can choose other values\n",
        "N_LAYERS = 2  # you can choose other values\n",
        "DROPOUT = 0.2  # you can choose other values\n",
        "criterion = nn.CrossEntropyLoss()  # maps the output of a Linear layer to a probability distribution\n",
        "                                   # see https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html\n",
        "N_TOKENS = len(corpus.dictionary)  # length of the dictionary\n",
        "model = LSTMModel(N_TOKENS, EMBEDDING_SIZE, HIDDEN_SIZE, N_LAYERS, DROPOUT).to(device)  # initialize the LSTM model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1mybRNEeXQX1"
      },
      "outputs": [],
      "source": [
        "# You do not need to thoroughly understand the contents of this cell.\n",
        "# However, some resources are available in the comments if you are curious.\n",
        "# The values are not necessarily optimal, by the way;\n",
        "# you could try to tune them later if you have the time.\n",
        "\n",
        "LR = 10 # https://towardsdatascience.com/understanding-learning-rates-and-how-it-improves-performance-in-deep-learning-d0d4059c1c10\n",
        "OPTIMIZER = 'sgd'  # https://ruder.io/optimizing-gradient-descent/\n",
        "WDECAY = 0  # https://d2l.ai/chapter_multilayer-perceptrons/weight-decay.html\n",
        "CLIP = 0.25  # https://neptune.ai/blog/understanding-gradient-clipping-and-how-it-can-fix-exploding-gradients-problem\n",
        "if OPTIMIZER == 'sgd':\n",
        "    optim = torch.optim.SGD(model.parameters(), lr=LR, weight_decay=WDECAY)\n",
        "elif OPTIMIZER == 'adam':\n",
        "    optim = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=WDECAY)\n",
        "else:\n",
        "    optim = None\n",
        "\n",
        "if OPTIMIZER in ['sgd', 'adam']:\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optim, step_size=1, gamma=0.7)\n",
        "else:\n",
        "    scheduler = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U7yI-GVbdF-o"
      },
      "outputs": [],
      "source": [
        "# Other global parameters\n",
        "EPOCHS = 10  # number of rounds of training, you can choose other values\n",
        "SEQ_LEN = 30  # length of the sequences, you can choose other values\n",
        "LOG_INTERVAL = 100  # for logging purposes, you can choose other values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wxhr9sULycPz",
        "tags": []
      },
      "source": [
        "### Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ySLy7eLyY2m"
      },
      "outputs": [],
      "source": [
        "def repackage_hidden(h):\n",
        "    if isinstance(h, torch.Tensor):\n",
        "        return h.detach()\n",
        "    else:\n",
        "        return tuple(repackage_hidden(v) for v in h)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0CFD9d9_nAC9"
      },
      "outputs": [],
      "source": [
        "def train():\n",
        "    model.train()\n",
        "    total_loss = 0.\n",
        "    start_time = time.time()\n",
        "    hidden = model.init_hidden(BATCH_SIZE)\n",
        "\n",
        "    # for batch, i in enumerate(range(0, train_data.size(0) - 1, SEQ_LEN)):\n",
        "        data, targets = get_batch(train_data, i, SEQ_LEN)\n",
        "\n",
        "        if optim is not None:\n",
        "            optim.zero_grad()\n",
        "        hidden = repackage_hidden(hidden)\n",
        "\n",
        "        output, hidden = model(data, hidden)\n",
        "\n",
        "        loss = criterion(output.view(-1, N_TOKENS), targets.view(-1))\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
        "        if CLIP is not None:\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), CLIP)\n",
        "\n",
        "        if optim is None:\n",
        "            for p in model.parameters():\n",
        "                p.data.add_(p.grad, alpha=-lr)\n",
        "        else:\n",
        "            optim.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        if batch % LOG_INTERVAL == 0 and batch > 0:\n",
        "            cur_loss = total_loss / LOG_INTERVAL\n",
        "            elapsed = time.time() - start_time\n",
        "            if optim is None:\n",
        "                print('| epoch {:3d} | {:5d}/{:5d} batches | lr {:02.4f} | ms/batch {:5.2f} | '\n",
        "                        'loss {:5.2f} | ppl {:8.2f}'.format(\n",
        "                    epoch, batch, len(train_data) // SEQ_LEN, lr,\n",
        "                    elapsed * 1000 / LOG_INTERVAL, cur_loss, math.exp(cur_loss)))\n",
        "            else:\n",
        "                print('| epoch {:3d} | {:5d}/{:5d} batches | lr {:02.4f} | ms/batch {:5.2f} | '\n",
        "                        'loss {:5.2f} | ppl {:8.2f}'.format(\n",
        "                    epoch, batch, len(train_data) // SEQ_LEN, scheduler.get_last_lr()[-1],\n",
        "                    elapsed * 1000 / LOG_INTERVAL, cur_loss, math.exp(cur_loss)))\n",
        "            total_loss = 0\n",
        "            start_time = time.time()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XTmS1aIGyeRA"
      },
      "outputs": [],
      "source": [
        "def evaluate(source):\n",
        "    model.eval()\n",
        "    total_loss = 0.\n",
        "    hidden = model.init_hidden(EVAL_BATCH_SIZE)\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, source.size(0) - 1, SEQ_LEN):\n",
        "            data, targets = get_batch(source, i, SEQ_LEN)\n",
        "            output, hidden = model(data, hidden)\n",
        "            hidden = repackage_hidden(hidden)\n",
        "            total_loss += len(data) * criterion(output.view(-1, N_TOKENS), targets.view(-1)).item()\n",
        "    return total_loss / (len(source) - 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TGmq3kUzaaQO"
      },
      "outputs": [],
      "source": [
        "def generate(source, n_words, temperature=1, topk=10):\n",
        "    \"\"\"\n",
        "        n_words: number of words to generate\n",
        "        fout: optional output file\n",
        "    \"\"\"\n",
        "    vocab_to_int = corpus.dictionary.word2idx\n",
        "    int_to_vocab = corpus.dictionary.idx2word\n",
        "    model.eval()\n",
        "    softmax = nn.Softmax(dim=-1)\n",
        "    source = source.split()\n",
        "    hidden = model.init_hidden(1)\n",
        "    for v in hidden:\n",
        "        v = v.to(device)\n",
        "    for w in source:\n",
        "        ix = torch.tensor([[vocab_to_int[w]]]).to(device)\n",
        "        output, hidden = model(ix, hidden)\n",
        "    output = output / temperature\n",
        "    if topk > 0:\n",
        "        probas = softmax(torch.topk(softmax(output[0]), topk).values[0]).cpu().detach().numpy()\n",
        "        indices = torch.topk(softmax(output[0]), topk).indices[0].cpu()\n",
        "        idx_max = np.random.choice(indices, 1, p=probas)[0]\n",
        "    else:\n",
        "        idx_max = torch.argmax(softmax(output[0]))\n",
        "    words = []\n",
        "    words.append(int_to_vocab[idx_max])\n",
        "    for i in range(1, n_words):\n",
        "        ix = torch.tensor([[idx_max]]).to(device)\n",
        "        output, hidden = model(ix, hidden)\n",
        "        output = output / temperature\n",
        "        if topk > 0:\n",
        "            probas = softmax(torch.topk(softmax(output[0]), topk).values[0]).cpu().detach().numpy()\n",
        "            indices = torch.topk(softmax(output[0]), topk).indices[0].cpu()\n",
        "            idx_max = np.random.choice(indices, 1, p=probas)[0]\n",
        "        else:\n",
        "            idx_max = torch.argmax(softmax(output[0]))\n",
        "        words.append(int_to_vocab[idx_max])\n",
        "    text = \" \".join(words)\n",
        "    text = text.replace(\"<eos>\", \"\\n\")\n",
        "    pp.pprint(text)\n",
        "    return words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SiOwJgcNs4BS"
      },
      "source": [
        "**Questions** whose answers will help you better understand what is happening:\n",
        "1. What are `model.train()` and `model.eval()` for?\n",
        "2. What is the `hidden` variable in the `train()` and `evaluate()` functions?\n",
        "3. What is `loss.backward()` in `train()` for?\n",
        "4. What is `optim.step()` for?\n",
        "5. What is `repackage_hidden()` for?\n",
        "6. How does the `generate()` function work?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Bb_sSBfvKrz"
      },
      "source": [
        "### Training loop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDvRxyozSppL"
      },
      "source": [
        "⚠ Now is a good time to switch to a GPU runtime. You might need to run all cells again; make sure that the output of the cell with the `device` variable is `cuda`. ⚠"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AsYR1wZyvlJM"
      },
      "outputs": [],
      "source": [
        "MODEL_NAME = 'model2'\n",
        "SAVE_PATH = os.path.join(CHECKPOINT_PATH, '{}.pt'.format(MODEL_NAME))\n",
        "LOAD_EXISTING_MODEL = True  # Change to False if you want to start from scratch\n",
        "                            # ⚠ If False, any existing model with the same name\n",
        "                            # will be overwritten ⚠\n",
        "\n",
        "if LOAD_EXISTING_MODEL:\n",
        "    try:\n",
        "        with open(SAVE_PATH, 'rb') as f:\n",
        "            model = torch.load(f)\n",
        "            # after load the rnn params are not a continuous chunk of memory\n",
        "            # this makes them a continuous chunk, and will speed up forward pass\n",
        "            model.rnn.flatten_parameters()\n",
        "        print(\"Successfully loaded model from {}\".format(SAVE_PATH))\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "best_val_loss = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ztXB2h-vHET"
      },
      "outputs": [],
      "source": [
        "# Loop over epochs.\n",
        "# At any point you can hit Ctrl + C to break out of training early.\n",
        "try:\n",
        "    lr = LR\n",
        "    for epoch in range(1, EPOCHS+1):\n",
        "        epoch_start_time = time.time()\n",
        "        train()\n",
        "        val_loss = evaluate(val_data)\n",
        "        if scheduler is not None:\n",
        "            scheduler.step()\n",
        "        print('-' * 89)\n",
        "        print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
        "                'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
        "                                           val_loss, math.exp(val_loss)))\n",
        "        print('-' * 89)\n",
        "\n",
        "        # Save the model if the validation loss is the best we've seen so far.\n",
        "        if not best_val_loss or val_loss < best_val_loss:\n",
        "            with open(SAVE_PATH, 'wb') as f:\n",
        "                torch.save(model, f)\n",
        "            print(\"Successfully saved model at {}\".format(SAVE_PATH))\n",
        "            best_val_loss = val_loss\n",
        "        else:\n",
        "            if scheduler is None:\n",
        "                lr /= 4.0\n",
        "\n",
        "except KeyboardInterrupt:\n",
        "    print('-' * 89)\n",
        "    print('Exiting from training early')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uCc0XAKRzJHX"
      },
      "outputs": [],
      "source": [
        "# Load the best saved model.\n",
        "with open(SAVE_PATH, 'rb') as f:\n",
        "    model = torch.load(f)\n",
        "    # after load the rnn params are not a continuous chunk of memory\n",
        "    # this makes them a continuous chunk, and will speed up forward pass\n",
        "    model.rnn.flatten_parameters()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zB9ULQAq2qZD"
      },
      "outputs": [],
      "source": [
        "# Run on test data.\n",
        "test_loss = evaluate(test_data)\n",
        "print('=' * 89)\n",
        "print('| End of training | test loss {:5.2f} | test ppl {:8.2f}'.format(\n",
        "    test_loss, math.exp(test_loss)))\n",
        "print('=' * 89)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SyGu-GRo5Odu"
      },
      "outputs": [],
      "source": [
        "# Generate some text\n",
        "\n",
        "# TODO, use the `generate` function\n",
        "pass"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}